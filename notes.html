<h1>Risk and Responsibility</h1>

<h2>The Risk & Responsibility theme considers the following material:</h2>

<ul>
    <li>Professional bodies</li>
    <li>Display Screen Regulations</li>
    <li>Risk</li>
    <li>Consultancy and contract hire</li>
    <li>Contracts</li>
    <li>Security through contracts</li>
    <li>Defective software</li>
    <li>Computer Misuse</li>
    <li>Plan, Do, Check, Act</li>
</ul>
<h2>Professional bodies</h2>

Professionals (e.g., architects, engineers, medics) must accept responsibility for their actions, especially when a mistake could be disastrous.
<br>
A professional body is an organisation that promotes high standards in a particular profession.
<br>

<h3>Examples of IT-related professional bodies:</h3>

<ul>
    <li>ACM (US-based but international membership)</li>
    <li>BCS (UK)</li>
    <li>IEEE (US-based but international membership; electrical/telecommunications/computer engineering)</li>
    <li>IET (UK; electrical/computer engineering)</li>
</ul>
Each professional body publishes a code of conduct, and requires its members to adhere to it.
<br>

The professional body can suspend or expel members for serious breaches of its code of conduct.
<br>

In some professions (e.g., medicine), suspension or expulsion entails loss of the right to practice.
<br>

In software engineering, this is not (yet) the case.
<br>

<h2>Display Screen Equipment</h2>

<ul>
    <li>Regulations apply to DSE users</li>

    <ul>
        <li>DSE users are employees that use display screen equipment continuously for an hour or more in a single working day.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Regulations do not apply to employees that use DSE for short periods of time or use it infrequently.</li>
</ul>

<ul><li>Display Screen Equipment (DSE) are equipment or devices that contains or includes a display.</li>

    <ul>
        <li>Alphanumeric display</li>
        <li>Graphical display</li>
        <li>Laptops</li>
        <li>Touch screens.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>DSE users are exposed to potential health risks</li>

    <ul>
        <li>Fatigue</li>
        <li>Eye strain</li>
        <li>Soar limbs or back ache</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Employees can be exposed to such risks through DSE devices and equipment.</li>

    <ul>
        <li>Overuse</li>
        <li>Improper use</li>
        <li>Badly designed workstations</li>
        <li>Poorly developed environments</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Employers can work with DSE users to avoid potential health risks.</li>

    <ul>
        <li>Identify risks</li>
        <li>Develop and implement practical controls</li>
        <li>Commit to healthy working.</li>
        <li>Employers should consult and communicate with DSE regards issues as well as providing relevant information.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Employers should consider:</li>

    <ul>
        <li>Special requirements of the employee.</li>
        <li>Role of employee or task being performed.</li>
        <li>Environment, not just the DSE itself.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Employers do not need to ask employees to assess risk for temporary home working, but employees should inform employers of any discomfort.</li>
</ul>

<ul><li>Employers should support employees in conducting complete risk assessments for long-term or permanent home working arrangements.</li>
</ul>

<ul><li>Regulations operate from the premise that DSE work will not result in permanent damage to eyesight or eyes.</li>
</ul>

<ul><li>DSE users can request eye tests, employers are expected to provide tests and supply eyewear (if necessary for DSE work).</li>
</ul>

<ul><li>Employers may dictate that employees access a nominated optician or they may opt to allow employees to provide bill for expense.</li>
</ul>

<ul><li>Employers under the regulations, must:</li>

    <ul>
        <li>Analyse workstations and assess as well as reduce risk.</li>
        <li>Ensure appropriate controls are in place.</li>
        <li>Ensure employees have sufficient information and training.</li>
        <li>Provide eyesight tests upon request, special eyewear if required.</li>
        <li>Review the assessment.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Employers must assess risk when:</li>

    <ul>
        <li>DSE user complain.</li>
        <li>DSE user commences a new role/position.</li>
        <li>Environmental change impacts interaction with DSE.</li>
        <li>New DSE is provided to employee.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>DSE assessment must be reviewed, when:</li>

    <ul>
        <li>Users change workstations.</li>
        <li>The nature of work tasks changes considerably.</li>
        <li>The current controls in place may be producing problems for the user.</li>
        <li>Significant changes occur to the equipment, environment or system.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Health and Safety Executive provide the DSE workstation checklist.</li>
</ul>

<ul><li>The checklist supports organisations in completing a risk assessment and complying with the legislation.</li>
</ul>

<ul><li>The DSE checklist assessment outlines a number of risks and other factors.</li>

    <ul>
        <li>Keyboards, mouse and trackballs.</li>
        <li>Display screens.</li>
        <li>Software considerations.</li>
        <li>Furniture.</li>
        <li>Environment.</li>
    </ul></li>
</ul>
<h2>Risk Management</h2>

<h3>Risk Management, Outcomes of Risk Management, and Risk Types</h3>

<ul>
    <li>Risk Management</li>

    <ul>
        <li>activities used to coordinate efforts and employees with regards to risk.</li>
        <li>risk management process should be built atop a framework and principles.</li>
        <li>framework should supporting integrating the risk management process itself in the management processes for an enterprise.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Outcomes of Risk Management</li>

    <ul>
        <li>intolerable risk, elements need to abandoned, replaced or evolved to reduce vulnerabilities.</li>
        <il>Example: A new AI algorithm poses significant privacy risks that cannot be mitigated. The company decides to abandon its development. </il>
        <li>tolerable risk, risks have been reduced using solutions to as long as reasonably possible (ALARP).</li>
        <il>Example: A cloud service provider acknowledges the risk of downtime but has mitigated it with robust backup systems, accepting a minimal level of risk due to natural disasters.</il>
            <li>acceptable risk, risk reduction not needed - upside risk that companies not only accept but embrace.</li>
            <il>Example: A tech startup invests in a risky but innovative project, understanding that the potential high reward justifies the risk.</il>
    </ul></ul>

<ul><ul>
</ul>
    <li>Risk Types</li>

    <ul>
        <li>routine risks, normal decision process that make use of statistics and data to inform decisions.</li>
        <il>Example: An IT manager regularly updates security protocols based on routine risk assessments of potential system vulnerabilities.</il>

        <li>complex risks, less obvious may need to gather more evidence and perform cost analysis.</li>
        <il> it needs more detailed discussion </il>
        <il>Example: Deciding whether to integrate a new, untested technology into existing systems, requiring detailed analysis of potential impacts and costs.</il>
        <li>uncertain risks, lack of predictability and need to monitor impact and roll back any solutions.</li>
        <il>Example: A company decides to use a new AI algorithm to improve its customer service, but the algorithm is not yet fully understood and may have unintended consequences.</il>
        <li>ambiguous risks, stakeholders interpret risk differently, need to ensure participatory decision making.</li>
        <il>Interpretation of these risks are depend on interpreter.</il>
        <il>Example: Like house price are ambiguous. Different experts can come up with different price ideas.</il>
        <il>So the data that is not very clear.</il>
    </ul></li>
</ul>
<h2>Consultancy and contract hire</h2>

<h3>Contract hire: </h3>

A supplier provides a customer with the services of an agreed number of staff for an agreed period at agreed rates.
<br>
The customer is responsible for managing the staff.
<br>
The supplier is responsible only for providing staff with the right competences.
<br>

<h3>Freelances </h3>

Freelances are individuals who contract out their own services
<br>
special case of contract hire.
<br>

<h3>Consultants</h3>

Consultants are experts who are contracted to advise customers on their operations or projects
<br>
up-market contract hire.
<br>

<h3>Comparing Contract Types</h3>

Contract-hire and consultancy contracts are much simpler than bespoke software contracts.
<br>

They should address:
<br>
<ul>
    <li>IPR</li>
    <li>confidentiality</li>
    <li>terms of reference</li>
    <li>liability (is the customer or supplier responsible for any loss?)</li>
</ul>

<h2>Contracts</h2>

A contract is an agreement between two or more parties that can be enforced in court.
<br>
<ul>
    <li>The contract may be verbal or written.</li>
    <li>The parties may be individuals or organisations.</li>
    <li>All parties must be competent to make a contract.</li>
    <li>Each party must receive something and provide something.</li>
</ul>
Until recently, existing contract law was adequate to cover contracts for supply of computer hardware and software.
<br>

But e-commerce needs new legal provisions, to cover transactions where the buyer and seller are in different countries.
<br>

<h3>Bespoke Development Contracts</h3>

Bespoke software is developed by a supplier for the exclusive use of one customer (or a consortium of customers).
<br>

The contract specifies the requirements and all deliverables (software, documentation, training, etc.).
<br>

Problem:
<br>
<ul>
    <li>Requirements are always changing.</li>
    <li>A contract is supposed to be definitive.</li>
</ul>
So the contract should explicitly allow for changing requirements at a price.
<br>

The contract should also define the customer's and supplier's IPR.
<br>

Copyright can apply only to code specifically developed for the customer:
<br>
<ul>
    <li>not to open-source code</li>
    <li>not to standard library code</li>
    <li>not to generic code previously developed by the supplier</li>
    <li>not to boilerplate code.</li>
</ul>
The contract might state that the supplier retains copyright but licences the customer to use it (perhaps exclusively).
<br>

The contract should state how the development project will be managed.
<br>

Each party should nominate a project manager, with authority to make decisions (including financial decisions up to some stated limit).
<br>

The contract should specify surcharges if:
<br>
<ul>
    <li>the customer is late in providing resources or information to the supplier</li>
    <li>the customer changes the requirements at a late stage.</li>
</ul>
The contract should specify an acceptance procedure:
<br>
<ul>
    <li>The customer must provide a set of test cases.</li>
    <li>If the system performs these test cases correctly, it is accepted.</li>
</ul>
<h3>Fixed-Price Contract</h3>
A fixed-price contract specifies exactly what the customer will pay, except for penalty clauses:
<br>
/ls
<br>
increasing the price if the customer is at fault (e.g., changing requirements)
<br>
decreasing the price if the supplier is at fault (e.g., unmet requirements, or late delivery).
<br>
/le
<br>

<h3>Cost-plus contract</h3>
A cost-plus contract specifies that the customer will pay the supplier's actual costs plus a profit margin.
<br>

Why consider a cost-plus contract?
<br>
<ul>
    <li>The supplier might be unwilling to undertake a fixed-price contract if the requirements are unclear.</li>
    <li>In a fixed-price contract, the supplier will add in a contingency allowance, which the customer must pay regardless.</li>
</ul>
<h2>Security Through Contracts</h2>

<h3>Overview</h3>

Contracts can be utilised as an approach to ensure security standards or specific security requirements.
<br>

Enterprises and other companies may rely on other partners or an extensive supply chain.
<br>

Legal forms could include specific conditions, warranties and/or third-party certification.
<br>

Caution should be exercised that enforcement may be prohibitive or costly.
<br>
at least by considering such requirements companies will need to perform due diligence that will still benefit the organisation.
<br>

<h3>Payment Platforms</h3>

Payment or trading platforms effectively represent a closed club where membership is maintained via contract.
<br>

Members must adhere to various rules surrounding many aspects of transactions.
<br>
<ul>
    <li>Duration and timing of transactions.</li>
    <li>Equipment utilised.</li>
    <li>Authentication protocols.</li>
</ul>
The platforms ensure standards and specifications via contracts, members must comply to ensure successful transactions and to collect payment.
<br>

The contracts utilised by payment platforms will typically reflect national and international rules and regulations.
<br>

Failure to comply, may result in failure to collect payment.
<br>

<ul>
    <li>If a member fails to comply with the contract when conducting a transaction, for example not adhering to security standards, they may jeopardise payment.</li>
</ul>
The payment or trading platform could deny payment, even if a transaction has completed, if requirements of the contract have not been met.
<br>
<ul>
    <li>Member of the club uses the platform to attain payment for sale and delivery of goods.</li>
    <li>Customer pays for goods, payment collected via platform, member delivers goods to customer.</li>
    <li>Payment platform refuses to transfer money due to some failure in compliance during the transaction.</li>
</ul>
<h3>Payment Card Industry Data Security Standard (PCI DSS)</h3>

<ul>
    <li>Designed and developed to ensure consistent and secure use of cardholder data.</li>
    <li>Entitles that collect, store, process and transmit cardholder data typically need comply to the standard.</li>
</ul>

<ul><li>PCI DSS is a standard, not a law and so compliance is typically attained through contracts and other agreements.</li>

    <ul>
        <li>laws are still relevant, cardholder data is considered personal data and so in some cases data protections laws may be violated, e.g. data breach.</li>
        <li>For example: data breach involving cardholder data could result in fines under the PCI DSS and GDPR.</li>
    </ul></li>
</ul>
<h3>PCI DSS Six Control Objectives</h3>
<ul>
    <li>Build and Maintain Secure Network and Systems.</li>

    <ul>
        <li>Vendor supplied system configuration and defaults must not be used.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Protect Cardholder Data.</li>

    <ul>
        <li>Encrypt transmission of cardholder data and protect it.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Maintain Vulnerability Management Programme.</li>

    <ul>
        <li>Regularly update anti-virus software.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Implement Strong Access Control Measures.</li>

    <ul>
        <li>Restrict access to 'need-to-know-basis' and restrict physical access.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Regularly Monitor and Test Networks.</li>

    <ul>
        <li>Track and monitor cardholder data across network resources.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Maintain an Information Security Policy.</li>

    <ul>
        <li>Develop and refine security policy for staff and contractors.</li>
    </ul></li>
</ul>
<h3>PCI DSS versus Law</h3>

<ul>
    <li>PCI DSS is a standard, not a law and so compliance is typically attained through contracts and other agreements.</li>

    <ul>
        <li>laws are still relevant, cardholder data is considered personal data and so in some cases data protections laws may be violated, e.g. data breach.</li>
        <li>For example: data breach involving cardholder data could result in fines under the PCI DSS and GDPR.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>PCI DSS has specific truncation rules for the display of the primary account number (PAN) on receipts.</li>

    <ul>
        <li>"3.3 Mask PAN when displayed (the first six and last four digits are the maximum number of digits to be displayed), such that only personnel with a legitimate business need can see more than the first six/last four digits of the PAN."</li>
    </ul></li>
</ul>
<h3>US Fair and Accurate Credit Transactions Act (FACTA) 2003</h3>
Federal law designed to reduce identity fraud and providing citizens greater insight into their credit profile.
<br>

Section 113 outlines specific truncation rules regarding the display of the primary account number (PAN) on receipts.
<br>
<ul>
    <li>"G(1) Except as otherwise provided in this subsection, no person that accepts credit cards or debit cards for the transaction of business shall print more than the last 5 digits of the card number or the expiration date upon any receipt provided to the cardholder at the point of the sale or transaction."</li>
</ul>
<h3>Example: Microsoft Store</h3>
Carlo Guarisma sued Microsoft for providing excessive information on receipts from the Microsoft Store in Aventura Florida.
<br>

The receipt from the store contained the customer's name, salesperson and 10-digits from the payment card (the first six digits and the last four digits).
<br>

Microsoft were aligned with the DSS, but not with the law.
<br>
<ul>
    <li>PCI DSS also has additional caveats though, including that the standard does not supersede law and recall, there 'must be a legitimate business need.'</li>
</ul>
<h2>Defective Software</h2>
<h3>Overview</h3>

<ul>
    <li>All software contains faults (bugs).</li>
</ul>

<ul><li>Software suppliers try to limit their liability for any faults.Typically:</li>

    <ul>
        <li>If the software proves to be completely unusable, the supplier agrees to refund the purchase price.</li>
        <li>But contract/licence terms attempting to limit liability can be overridden by laws.</li>
    </ul></li>
</ul>
<h3>UK Unfair Contract Terms Act 1977</h3>

The UK Unfair Contract Terms Act 1977 makes liability-limiting terms enforceable in law only to the extent that they are reasonable.
<br>

A person injured as a result of faulty safety-critical software (or hardware) can sue the supplier for damages, regardless of any limitations stated in the contract/licence.
<br>

Even where software is not safety-critical, faults can cause significant economic damage. E.g.:
<br>
<ul>
    <li>an e-mail client that occasionally loses messages</li>
    <li>a mapping system that misplaces some commercial properties.</li>
</ul>
<h3>UK Sale of Goods Act 1979</h3>

The UK Sale of Goods Act 1979 requires that goods sold must be fit for purpose. This applies to retail software.
<br>
So a person who buys a software product in a shop but finds that it is seriously faulty, or does not do what it purports to do, can demand a refund.
<br>
<ul>
    <li>It is unclear whether this Act applies to software downloaded on the Internet!</li>
</ul>
<h2>Computer Misuse</h2>

<h3>Scope of computer misuse</h3>

<ul>
    <li>Computer misuse means unauthorised access to any computer.</li>
</ul>

<ul><li>This can be by any means:</li>
    <li>physical access</li>
    <li>via the Internet.</li>
    <li>This can be for any purpose:</li>
    <li>curiosity</li>
    <li>copying, modifying, or deleting data</li>
    <li>modifying programs</li>
    <li>disrupting normal operation.</li>
</ul>
<h3>The Computer Misue Act 1990 (CMA)</h3>

<ul>
    <li>The Computer Misuse Act 1990 (CMA) was enacted to combat misuse.</li>

    <ul>
        <li>Note: At that time Internet access was limited.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>CMA created three new criminal offences:</li>

    <ul>
        <li>unauthorised access to any program/data held in any computer</li>
        <li>as above, with intent to commit a serious offence</li>
        <li>unauthorised modification of the contents of any computer.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>CMA applies to:</li>

    <ul>
        <li>anyone anywhere who accesses a computer in UK</li>
        <li>anyone in UK who accesses a computer anywhere in the world.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Examples of offences:</li>

    <ul>
        <li>intentionally spreading a virus or worm</li>
        <li>modifying a company's or individual's web pages</li>
        <li>stealing sensitive personal data (e.g., for publication or blackmail)</li>
        <li>stealing commercially sensitive data</li>
        <li>disrupting a company's commercial operations</li>
        <li>disrupting operations of an agency concerned with health or safety (e.g., emergency services or air traffic control).</li>
    </ul></li>
</ul>
<h3>The Police and Justice Act 2006:</h3>
<ul>
    <li>increased maximum penalties for CMA offences</li>
    <li>amended CMA to cover intent to impair operation of any computer</li>
    <li>amended CMA to cover software tools intended to facilitate computer misuse.</li>
    <li>New offences:</li>

    <ul>
        <li>denial-of-service attacks</li>
        <li>building or selling hackers' toolkits.</li>
    </ul></li>
</ul><h3>Experience of computer misuse laws</h3>

<ul>
    <li>Convictions have been infrequent. Penalties have been lenient:</li>

    <ul>
        <li>never more than 3 years (even in the NoW phone hacking case)</li>

        <ul>
            <li>From Wikipedia:</li>
        </ul></ul></ul>

<ul><ul><ul>		<li>The News International phone hacking scandal was a controversy involving the now-defunct News of the World and other British newspapers owned by Rupert Murdoch. Employees of the newspaper engaged in phone hacking, police bribery, and exercising improper influence in the pursuit of stories.</li>
    <li>Investigations conducted from 2005 to 2007 showed that the paper's phone hacking activities were targeted at celebrities, politicians, and members of the British royal family. In July 2011 it was revealed that the phones of murdered schoolgirl Milly Dowler, relatives of deceased British soldiers, and victims of the 7 July 2005 London bombings had also been hacked. The resulting public outcry against News Corporation and its owner, Rupert Murdoch, led to several high-profile resignations, including that of Murdoch as News Corporation director, Murdoch's son James as executive chairman, Dow Jones chief executive Les Hinton, News International legal manager Tom Crone, and chief executive Rebekah Brooks. The commissioner of London's Metropolitan Police, Sir Paul Stephenson, also resigned. Advertiser boycotts led to the closure of the News of the World on 10th of July 2011, after 168 years of publication.[1] Public pressure forced News Corporation to cancel its proposed takeover of the British satellite broadcaster BSkyB.</li>
</ul></ul></ul>

<ul><ul><ul>
</ul>

</ul>
    <li>The prime minister, David Cameron, announced on 6 July 2011 that a public inquiry, known as the Leveson Inquiry, would look into phone hacking and police bribery by the News of the World and consider the wider culture and ethics of the British newspaper industry, and that the Press Complaints Commission would be replaced "entirely".[1][2] A number of arrests and convictions followed, most notably of the former News of the World managing editor Andy Coulson.</li>

    <ul>
        <li>maximum penalty is 10 years.</li>

    </ul>
    <li>Police are over-stretched. To investigate suspected offences, they must engage (expensive) security experts.</li>
    <li>Companies that are attacked often prefer to keep quiet, to avoid drawing attention to their security weaknesses.</li>
</ul>
<h3>Warranted Activity</h3>

Exemption from the CMA and other acts may be permitted if the actions relate to state defence or criminal investigation.
<br>

Actions taken that have state authorisation, e.g. warrant, would be exempted from specific laws.
<br>

Liability for actions may be exempted to the extent that is expressed by the state authorisation, i.e. want the warrant permits.
<br>

Example: warrant issued in line with the (UK) Investigatory Powers Act (IPA) 2016 would be exempt from criminal liability under the (UK) Computer Misuse Act (CMA) 1990.
<br>

<h3>Computer Fraud</h3>

<ul>
    <li>Computer Fraud and Abuse Act 1986 covers:</li>

    <ul>
        <li>unauthorised access to any 'protected computer'</li>
        <li>distribution of malicious code</li>
        <li>denial-of-service attacks</li>
        <li>trafficking in passwords.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>It is draconian:</li>

    <ul>
        <li>Penalties for first offences up to 10 years.</li>
        <li>Penalties for repeat offences up to 20 years.</li>
        <li>Some offences are treated as terrorism.</li>
        <li>In UK, computer fraud is covered by existing anti-fraud laws.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>But the Internet makes fraud much easier:</li>

    <ul>
        <li>on-line banking</li>
        <li>e-commerce.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Also, detection and conviction are harder:</li>

    <ul>
        <li>Collection and preservation of evidence of computer fraud requires specialised expertise.</li>
        <li>Trials require specialist experts as witnesses.</li>
    </ul></li>
</ul>
<h2>Plan, Do, Check, Act</h2>

<ul>
    <li>general management approach for continuous improvement.</li>
    <li>understand the problem by collecting and analysing data, devise a plan to address it.</li>
    <li>develop a solution to the problem and deploy it, collect measurements to understand effectiveness.</li>
    <li>check that solution actually addresses the perceived problem.</li>
    <li>produce report, communicate changes and identify the next set of problems</li>
</ul>

<div>
    <h2>Plan-Do-Check-Act (PDCA) Cycle</h2>
    <p>The PDCA cycle is a well-established framework for continuous improvement in management. It consists of four iterative steps:</p>
    <ol>
        <li><strong>Plan:</strong> Identify areas of improvement and develop strategies to address these issues. This involves understanding the problem and planning a solution.</li>
        <li><strong>Do:</strong> Implement the solution. This step includes taking action, allocating resources, and organizing necessary assets to achieve the planned outcome.</li>
        <li><strong>Check:</strong> Evaluate the effectiveness of the solution. This involves collecting and analyzing data to ascertain if the solution is working as intended and to identify any unintended consequences.</li>
        <li><strong>Act:</strong> Report and communicate the results. This step includes informing others about the outcomes and learning from the experience for future improvements.</li>
    </ol>
    <p>The cycle is iterative, meaning that it begins again at the planning stage after the act stage to address new problems, improve upon the solution, or handle unintended consequences, thereby ensuring continuous improvement.</p>
</div>
<h1>Decisions and Discrimination</h1>

<h2>Overview</h2>

The Decisions & Discrimination theme considers the following material:
<br>
<ul>
    <li>Profiling</li>
    <li>Automated decision-making</li>
    <li>Data Protection Impact Assessment</li>
    <li>Anti-discrimination</li>
    <li>Web Content Accessibility Guidelines</li>
    <li>Automated decision-making, Data Protection and Equality</li>
</ul>
<h2>Profiling</h2>

<h3>Profiling:</h3>
Extracting patterns through automated processing of large volumes of personal data.
<br>
Using algorithms and techniques to analyse aspects of individuals.
<br>
Aspects that include habits, interests, behaviour and personality.
<br>
These patterns form profiles that can then be incorporated into a decision-making process.
<br>

<h3>Data sources</h3>

Organisations can generate these profiles from a range of data sources, including:
<br>
<ul>
    <li>social networks</li>
    <li>video surveillance</li>
    <li>web browsing history</li>
    <li>Internet of Things.</li>
</ul>
<h3>Motivation for organisations</h3>

Profiles can be utilised as part of the decision making process by organisations to:
<br>
<ul>
    <li>make decisions about individuals</li>
    <li>anticipate behaviour</li>
    <li>determine individual preferences.</li>
</ul><h3>Applications</h3>
<ul>
    <li>Apply generated profiles to individuals.</li>
    <li>Predict behaviours of an individual.</li>
    <li>Profiles can be used for tasks such as marketing, but also to support serious decisions.</li>
    <li>Consider the Harm Assessment Risk Tool (HARM) example from the seminar.</li>

    <ul>
        <li>https://moodle.gla.ac.uk/pluginfile.php/7708066/mod_resource/content/1/DD%20Seminar.pdf</li>
        <li>Page 29</li>
    </ul></li>
</ul>
<h3>Harm Assessment Risk Tool (HART)</h3>
<ul>
    <li>Positioned as an artificial intelligence tool to support custodial decisions.</li>
    <li>Deployed and being used by Durham Constabulary.</li>
    <li>Relies upon the histories of 104,000 of individuals considered and arrested in Durham in the last five years.</li>
    <li>The tool provides a risk assessment of the likelihood of the individual in question commence an offence again in the next two years.</li>
</ul>
<h2>Automated Decision Making</h2>

<div>
    <ol>
        <li>
            <strong>Definition and Process</strong>:
            <ul>
                <li>Occurs largely without human intervention but includes human roles like data input or final decision-making.</li>
                <li>Example: Law enforcement tools providing arrest likelihoods for police officer decisions.</li>
            </ul>
        </li>
        <li>
            <strong>Human Involvement</strong>:
            <ul>
                <li>Process involves humans in stages like data entry or final decision-making.</li>
            </ul>
        </li>
        <li>
            <strong>Mechanisms and Data Use</strong>:
            <ul>
                <li>Systems may use various data sources and profiles; usage varies.</li>
                <li>Decision-making process and data utilization can be opaque.</li>
            </ul>
        </li>
        <li>
            <strong>Benefits</strong>:
            <ul>
                <li>Efficiency in rapid and accurate decision-making.</li>
                <li>Practical for handling large data volumes and fast processing.</li>
            </ul>
        </li>
        <li>
            <strong>Concerns and Impact</strong>:
            <ul>
                <li>Lack of transparency in the decision-making process and data use.</li>
                <li>Significant effects on individuals in areas like employment, loans, or benefits.</li>
                <li>Limited insight for individuals into the decision-making and data handling.</li>
            </ul>
        </li>
        <li>
            <strong>Societal Implications</strong>:
            <ul>
                <li>Notable impact on individuals and society, with concerns over process understanding and data management.</li>
            </ul>
        </li>
    </ol>
</div>




Decision process without human involvement.
<br>

Decision process can rely upon on profiles, inferences and other data.
<br>

Automated decision-making could use profiling, but not always.
<br>

Automated decision-making can result in more accurate and rapid decisions.
<br>

Affords rapid decisions that involves analysis of large volumes of data.
<br>

The decision process is often not transparent to individuals.
<br>
<ul>
    <li>Lack of insight into how data is used to make decisions.</li>
    <li>Impact from automated decisions could have a significant impact on an individual.</li>
</ul>




<div>
    <h2>Data Protection Impact Assessment (DPIA) Summary</h2>
    <ol>
        <li><strong>Definition and Purpose:</strong> A process for organizations to document and consider the implications and risks of processing personal data.</li>
        <li><strong>When Required:</strong> Mandatory when data processing poses a high risk.</li>
        <li><strong>Good Practice:</strong> Advisable for large projects involving personal data processing.</li>
        <li><strong>Key Considerations:</strong>
            <ul>
                <li><em>Context:</em> Data collection, storage, usage, and sharing.</li>
                <li><em>Scope:</em> Nature, volume, sensitivity, and frequency of data.</li>
                <li><em>Nature:</em> Source of data and organization-individual relationship.</li>
                <li><em>Purpose:</em> Intended outcome for individuals and society.</li>
            </ul>
        </li>
        <li><strong>Risk Assessment:</strong> Identifying and mitigating potential risks to individuals.</li>
        <li><strong>Compliance and Necessity:</strong> Ensuring processing aligns with regulations and is necessary for the intended outcome.</li>
        <li><strong>Consultation:</strong> Involving affected individuals or their representatives.</li>
        <li><strong>Risks Identification and Assessment:</strong> Determining potential risks, their severity, and likelihood.</li>
        <li><strong>Mitigation Solutions:</strong> Developing strategies to address identified risks.</li>
        <li><strong>Documentation:</strong> Keeping a record of all DPIA considerations and actions.</li>
        <li><strong>Publishing DPIA:</strong> Not mandatory, but considered good practice.</li>
        <li><strong>High-Risk Situations:</strong> Reporting unmitigable high risks to the national data protection authority.</li>
    </ol>
</div>


<ul>
    <li>context, scope, nature and purpose of data processing</li>
    <li>risks to individuals</li>
    <li>mitigation measures for those risks</li>
    <li>assess compliance, proportionality and necessity of processing.</li>

</ul>
<li>DPIA is required if you meet the requirements of Article 22.</li>

<ul>
    <li>DPIA can also be used to determine if processing does come under Article 22.</li>

</ul>
<li>Determine the risks of the profiling and automated decision-making process and the mitigation to those risks.</li>
<li>Publish the DPIA, but this is not required.</li>
<li>If you identify high risks that you can not mitigate then the DPIA must be submitted to the ICO (in the UK context).</li>

<ul>
    <li>The ICO can advise of the income, including prohibiting the intended processing.</li>
</ul></li>
</ul>
<h2>Anti-Discrimination</h2>


<div>
    <ol>
        <li><strong>Definition of Discrimination</strong>
            <ul>
                <li>Discrimination: Treating a person or group less favorably than others based on personal characteristics.</li>
            </ul>
        </li>
        <li><strong>UK Equality Act 2010 Overview</strong>
            <ul>
                <li>Consolidation of previous legislations like the Race Relations Act, Sex Discrimination Act, Disability Discrimination Act.</li>
                <li>Protects individual rights and promotes equality.</li>
            </ul>
        </li>
        <li><strong>Protected Characteristics under the Act</strong>
            <ul>
                <li>Age</li>
                <li>Disability</li>
                <li>Gender reassignment</li>
                <li>Marriage and civil partnership</li>
                <li>Pregnancy and maternity</li>
                <li>Race (including color, ethnic or national origins)</li>
                <li>Religion or belief</li>
                <li>Sex</li>
                <li>Sexual orientation</li>
            </ul>
        </li>
        <li><strong>Types of Discrimination</strong>
            <ul>
                <li><em>Direct Discrimination</em>: Different treatment because of a protected characteristic.</li>
                <li><em>Indirect Discrimination</em>: Policies that apply to all but adversely affect those with a protected characteristic.</li>
            </ul>
        </li>
        <li><strong>Permitted Discrimination</strong>
            <ul>
                <li>Legally allowed if it is a proportionate means to achieve a legitimate aim (e.g., health and safety).</li>
                <li>Can be challenged in court.</li>
            </ul>
        </li>
        <li><strong>Examples of Discrimination Cases</strong>
            <ul>
                <li>Sex: Discrimination based on sex or marital status, with certain occupational exceptions.</li>
                <li>Race: Illegal discrimination based on race, replaced by the Equality Act 2010.</li>
                <li>Age: Includes issues like compulsory retirement age and employment programs affecting certain age groups.</li>
                <li>Disability: Illegal discrimination based on disability, employers required to make reasonable adjustments.</li>
            </ul>
        </li>
        <li><strong>Expectations for Employers</strong>
            <ul>
                <li>Make reasonable adjustments for disabilities.</li>
                <li>Need to comply with Web Content Accessibility Guidelines.</li>
                <li>Provide accessible software and systems.</li>
            </ul>
        </li>
        <li><strong>Legal Context and Enforcement</strong>
            <ul>
                <li>Permitted discrimination can be contested in court.</li>
                <li>Importance of proportionality and legitimacy in permitted discrimination.</li>
            </ul>
        </li>
        <li><strong>Role of Technology</strong>
            <ul>
                <li>Software engineers and system designers should ensure accessibility for all users, including those with disabilities.</li>
            </ul>
        </li>
    </ol>
</div>




<h3>The (UK) Equality Act 2010</h3>

The (UK) Equality Act 2010 aims to progress equality and protect the rights of individuals from discrimination.
<br>
<ul>
    <li>The Act incorporates and supersedes many prior pieces of legislation:</li>

    <ul>
        <li>the Equal Pay Act 1970</li>
        <li>the Sex Discrimination Act 1975</li>
        <li>the Race Relations Act 1976</li>
        <li>the Disability Discrimination Act 1995</li>
        <li>the Employment Equality (Religion or Belief) Regulations 2003</li>
        <li>the Employment Equality (Sexual Orientation) Regulations 2003</li>
        <li>the Employment Equality (Age) Regulations 2006</li>
        <li>the Equality Act 2006, Part 2</li>
        <li>the Equality Act (Sexual Orientation) Regulations 2007</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>(UK) Equality Act 2010 outlaws discrimination on protected characteristics</li>

    <ul>
        <li>age</li>
        <li>disability</li>
        <li>gender reassignment</li>
        <li>marriage and civil partnership</li>
        <li>pregnancy and maternity</li>
        <li>race, colour, ethnic origin or nationality</li>
        <li>religion or belief</li>
        <li>sex</li>
        <li>sexual orientation.</li>
    </ul></li>
</ul>
Discrimination can be categorised as direct or indirect.
<br>

<ul>
    <li>Direct discrimination is when an individual is treated differently than another because of a protected characteristic.</li>
    <li>Indirect discrimination is when a decision for everyone has a different impact on individuals that share a protected characteristic.</li>
</ul>
Discrimination can be permitted under the (UK) Equality Act 2010, if there is a legitimate reason.
<br>
<ul>
    <li>An individual can justify discrimination under the (UK) Equality Act 2010, if they can demonstrate it is a proportionate means to achieve a legitimate aim.</li>
    <li>Individuals can test if discrimination is permitted under the (UK) Equality Act 2010 in court.</li>
</ul>
<h3>Grounds for Dismissal</h3>

<ul>
    <li>Sex</li>

    <ul>
        <li>The (UK) Equality Act 2010 prohibits employers from discriminating against employees on the grounds of sex or marital  status.</li>
        <li>There are exceptions, e.g. where there is an occupational requirement for an individual of a specific sex.</li>

    </ul>
    <li>Race</li>

    <ul>
        <li>The (UK) Race Relations Act 1965 made it illegal to discriminate on the grounds of race.</li>
        <li>The (UK) Equality Act 2010 supersedes the legislation and consolidates laws to prohibit discrimination on the grounds of race.</li>

    </ul>
    <li>Age</li>

    <ul>
        <li>The (UK) Equality Act 2010 makes it illegal to discriminate on the basis of age.</li>
        <li>Compulsory retirement ages are not permitted and some programmes of employment may be prohibited.</li>
        <li>The Act does not apply to those under 18 years of age.</li>
        <li>Permitted discrimination in some cases, e.g. maximum and minimum recruitment ages.</li>

    </ul>
    <li>Disability</li>

    <ul>
        <li>The (UK) Disability Discrimination Act 1995 made it illegal to discriminate on the grounds of disability.</li>
        <li>The (UK) Equality Act 2010 supersedes the legislation and consolidates laws to prohibit discrimination on the grounds of disability.</li>
        <li>The (UK) Equality Act 2010 prohibits employers for not considering applicants on the grounds of disability without justification.</li>
        <li></li>
        <li>However, employers are not exempt from making reasonable adjustments to support the individual.</li>
        <li>Systems should be able to accommodate reasonable adjustments.</li>
        <li>Engineers need to consider how their system can be reasonably adjusted for those with different disabilities.</li>
    </ul></li>
</ul>
<br>
<h3>Public Sector Bodies (Websites and Mobile Applications) (No. 2) Accessibility Regulations 2018</h3>
The regulations require Public Sector Bodies to meet the requirements of the (UK) Equality Act 2010 in providing reasonable adjustments for disabled people.
<br>

The expectation is that the regulations can be met by complying with the Web Content Accessibility Guidelines (WCAG) and providing an accessibility statement.
<br>

<h3>Avoiding discrimination</h3>
Employers and employee must adhere to the (UK) Equality Act 2010.
<br>
Employers can support employees in attaining the act, via:
<br>
<ul>
    <li>policy</li>
    <li>effective procedures</li>
    <li>training and education.</li>
</ul>
<h2>Web Content Accesibility Guidelines</h2>


<div>

    <div>
        <h3>Introduction:</h3>
        <ul>
            <li>WCAG: Internationally recognized guidelines for accessible websites and applications.</li>
            <li>Aim: Ensure accessibility for all users, including those with disabilities.</li>
        </ul>
    </div>
    <div>
        <h3>Four Core Principles:</h3>
        <ol>
            <li>
                <strong>Perceivable:</strong>
                <ul>
                    <li><strong>Goal:</strong> Information and UI components must be presentable in ways users can perceive.</li>
                    <li><strong>Examples:</strong> Alternative text for images, captions for videos.</li>
                    <li><strong>Focus:</strong> Content should not be invisible to all senses of the user.</li>
                </ul>
            </li>
            <li>
                <strong>Operable:</strong>
                <ul>
                    <li><strong>Goal:</strong> UI components and navigation must be operable.</li>
                    <li><strong>Examples:</strong> Users must be able to interact with all elements.</li>
                    <li><strong>Focus:</strong> Ensuring interactive elements are accessible.</li>
                </ul>
            </li>
            <li>
                <strong>Understandable:</strong>
                <ul>
                    <li><strong>Goal:</strong> Information and operation of UI must be understandable.</li>
                    <li><strong>Examples:</strong> Clear instructions, error messages, understandable language.</li>
                    <li><strong>Focus:</strong> Make content and operations comprehensible.</li>
                </ul>
            </li>
            <li>
                <strong>Robust:</strong>
                <ul>
                    <li><strong>Goal:</strong> Content must be robust enough to be interpreted by a wide range of technologies, including assistive devices.</li>
                    <li><strong>Examples:</strong> Structured documents for easy parsing.</li>
                    <li><strong>Focus:</strong> Content compatibility with assistive technologies.</li>
                </ul>
            </li>
        </ol>
    </div>
    <div>
        <h3>Conformance Levels:</h3>
        <ol>
            <li>
                <strong>Level A (Minimal):</strong>
                <ul>
                    <li><strong>Goal:</strong> Basic accessibility features.</li>
                    <li><strong>Examples:</strong> Keyboard navigation, no keyboard traps.</li>
                </ul>
            </li>
            <li>
                <strong>Level AA (Acceptable):</strong>
                <ul>
                    <li><strong>Goal:</strong> More refined accessibility features.</li>
                    <li><strong>Examples:</strong> Consistent navigation elements, properly labeled headings.</li>
                </ul>
            </li>
            <li>
                <strong>Level AAA (Optimal):</strong>
                <ul>
                    <li><strong>Goal:</strong> Highest standard of accessibility.</li>
                    <li><strong>Examples:</strong> Sign language interpretation for videos, context-relevant help.</li>
                </ul>
            </li>
        </ol>
    </div>
    <div>
        <h3>Compliance:</h3>
        <ul>
            <li>Not all content may achieve Level AAA, but Level A is a minimum standard.</li>
            <li>Tools and metrics available for assessing conformance to these levels.</li>
        </ul>
    </div>
    <div>
        <h3>Conclusion:</h3>
        <ul>
            <li>WCAG principles and conformance levels guide the development of accessible digital content.</li>
            <li>Aim is to accommodate diverse user needs, ensuring wide accessibility.</li>
        </ul>
    </div>
</div>


https://www.w3.org/TR/WCAG21/ - link in moodle books
<br>

<br>
The WCAG are a set of guidelines that are internationally recognised (ISO/IEC 40500:2012).
<br>
The aim of the guidelines is to ensure that web solutions are accessible to a wide range of individuals with disabilities.
<br>
The guidelines are defined in such a way that they are not implementation specific.
<br>

<br>
Principles (lots copied from the link above!)
<br>

<ul>
    <li>Perceivable</li>

    <ul>
        <li>information and user interface components must be presentable to users in ways they can perceive.</li>

        <ul>
            <li>Provide text alternatives for any non-text content so that it can be changed into other forms people need, such as large print, braille, speech, symbols or simpler language.</li>

            <ul>
                <li>Controls, Input</li>

                <ul>
                    <li>If non-text content is a control or accepts user input, then it has a name that describes its purpose. (Refer to Success Criterion 4.1.2 for additional requirements for controls and content that accepts user input.)</li>
                </ul></ul></ul></ul></ul>

<ul><ul><ul><ul><ul>
</ul>
    <li>Time-Based Media</li>

    <ul>
        <li>If non-text content is time-based media, then text alternatives at least provide descriptive identification of the non-text content. (Refer to Guideline 1.2 for additional requirements for media.)</li>
    </ul></ul></ul></ul></ul>

<ul><ul><ul><ul><ul>
</ul>
    <li>Test</li>

    <ul>
        <li>If non-text content is a test or exercise that would be invalid if presented in text, then text alternatives at least provide descriptive identification of the non-text content.</li>
    </ul></ul></ul></ul></ul>

<ul><ul><ul><ul><ul>
</ul>
    <li>Sensory</li>

    <ul>
        <li>If non-text content is primarily intended to create a specific sensory experience, then text alternatives at least provide descriptive identification of the non-text content.</li>
    </ul></ul></ul></ul></ul>

<ul><ul><ul><ul><ul>
</ul>
    <li>CAPTCHA</li>

    <ul>
        <li>If the purpose of non-text content is to confirm that content is being accessed by a person rather than a computer, then text alternatives that identify and describe the purpose of the non-text content are provided, and alternative forms of CAPTCHA using output modes for different types of sensory perception are provided to accommodate different disabilities.</li>
    </ul></ul></ul></ul></ul>

<ul><ul><ul><ul><ul>
</ul>
    <li>Decoration, Formatting, Invisible</li>

    <ul>
        <li>If non-text content is pure decoration, is used only for visual formatting, or is not presented to users, then it is implemented in a way that it can be ignored by assistive technology.</li>

    </ul>

</ul>
    <li>Provide alternatives for time-based media</li>

    <ul>
        <li>videos, audio descriptions, etc. can have captions, sign language interpretations, etc.</li>
    </ul></ul></ul></ul>

<ul><ul><ul><ul>
</ul>
    <li>Adaptable</li>

    <ul>
        <li>Information, structure, and relationships conveyed through presentation can be programmatically determined or are available in text.</li>

        <ul>
            <li>If something must be read in a particular order, then the sequence must be programmatically determined.</li>

            <ul>
                <li>programmatic content is additional information that can be programmatically determined from relationships with a link, combined with the link text, and presented to users in different modalities</li>

            </ul>

        </ul>

    </ul>
    <li>Distinguishable</li>

    <ul>
        <li>Make it easier for users to see and hear content including separating foreground from background.</li>

    </ul>

</ul>

</ul>
    <li>Operable</li>

    <ul>
        <li>User interface components and navigation must be operable.</li>

        <ul>
            <li>Keyboard accessible</li>

            <ul>
                <li>All functionality of the content is operable through a keyboard interface without requiring specific timings for individual keystrokes, except where the underlying function requires input that depends on the path of the user's movement and not just the endpoints.</li>

            </ul>
            <li>If keyboard focus can be moved to a component of the page using a keyboard interface, then focus can be moved away from that component using only a keyboard interface, and, if it requires more than unmodified arrow or tab keys or other standard exit methods, the user is advised of the method for moving focus away.</li>

            <ul>
                <li>i.e. keyboard interfaces lose page context those using a keyboard would not lose, and takes away focus.</li>

            </ul>
            <li>Enough Time</li>

            <ul>
                <li>Provide users enough time to read and use content.</li>
                <li>or each time limit that is set by the content, at least one of the following is true:</li>

                <ul>
                    <li>Turn off</li>

                    <ul>
                        <li>The user is allowed to turn off the time limit before encountering it; or</li>
                    </ul></ul></ul></ul></ul></ul>

<ul><ul><ul><ul><ul><ul>
</ul>
    <li>Adjust</li>

    <ul>
        <li>The user is allowed to adjust the time limit before encountering it over a wide range that is at least ten times the length of the default setting; or</li>
    </ul></ul></ul></ul></ul></ul>

<ul><ul><ul><ul><ul><ul>
</ul>
    <li>Extend</li>

    <ul>
        <li>The user is warned before time expires and given at least 20 seconds to extend the time limit with a simple action (for example, "press the space bar"), and the user is allowed to extend the time limit at least ten times; or</li>
    </ul></ul></ul></ul></ul></ul>

<ul><ul><ul><ul><ul><ul>
</ul>
    <li>Real-time Exception</li>

    <ul>
        <li>The time limit is a required part of a real-time event (for example, an auction), and no alternative to the time limit is possible; or</li>

    </ul>
    <li></li>
    <li>Essential Exception</li>

    <ul>
        <li>The time limit is essential and extending it would invalidate the activity; or</li>
    </ul></ul></ul></ul></ul></ul>

<ul><ul><ul><ul><ul><ul>
</ul>
    <li>20 Hour Exception</li>

    <ul>
        <li>The time limit is longer than 20 hours.</li>

    </ul>

</ul>

</ul>
    <li>Pause, Stop, Hide</li>

    <ul>
        <li>No Timing</li>

        <ul>
            <li>Timing is not an essential part of the event or activity presented by the content, except for non-interactive synchronized media and real-time events.</li>
        </ul></ul></ul></ul></ul>

<ul><ul><ul><ul><ul>
</ul>
    <li>Interruptions</li>

    <ul>
        <li>Interruptions can be postponed or suppressed by the user, except interruptions involving an emergency.</li>

    </ul>
    <li>Re-authenticating</li>

    <ul>
        <li>When an authenticated session expires, the user can continue the activity without loss of data after re-authenticating.</li>

    </ul>
    <li>Timeouts</li>

    <ul>
        <li>Users are warned of the duration of any user inactivity that could cause data loss, unless the data is preserved for more than 20 hours when the user does not take any actions.</li>

    </ul>
    <li>Seizures and Physical Reactions</li>

    <ul>
        <li>Do not design content in a way that is known to cause seizures or physical reactions.</li>

    </ul>

</ul>

</ul>

</ul>
    <li>Understandable</li>

    <ul>
        <li>Information and the operation of the user interface must be understandable.</li>

        <ul>
            <li>content readable, understandable.</li>
            <li>default language of a page can be programatically determined</li>

        </ul>
        <li>All language can be deduced except names, places, etc.</li>
        <li></li>

    </ul>
    <li>Robust</li>

    <ul>
        <li>Content must be robust enough that it can be interpreted by a wide variety of user agents, including assistive technologies.</li>
        <li>Compatible</li>

        <ul>
            <li>Maximize compatibility with current and future user agents, including assistive technologies.</li>

        </ul>
        <li>Parsing</li>

        <ul>
            <li>In content implemented using markup languages, elements have complete start and end tags, elements are nested according to their specifications, elements do not contain duplicate attributes, and any IDs are unique, except where the specifications allow these features.</li>

        </ul>
        <li>Name, Role, Value</li>

        <ul>
            <li>For all user interface components (including but not limited to: form elements, links and components generated by scripts), the name and role can be programmatically determined; states, properties, and values that can be set by the user can be programmatically set; and notification of changes to these items is available to user agents, including assistive technologies.</li>

        </ul>
        <li>Status Messages</li>

        <ul>
            <li>In content implemented using markup languages, status messages can be programmatically determined through role or properties such that they can be presented to the user by assistive technologies without receiving focus.</li>
        </ul></li>
    </ul></li>
</ul>
<h2>Data protection</h2>

The (EU/UK) General Data Protection Regulations (GDPR) and (UK) Data Protection Act 2018 (DPA) regulate processing and collection of personal data.
<br>
<br>
Automated decision-making solutions that utilise personal data come within the scope of the legislation.
<br>
Automated decision-making solutions may also come with the scope of the (UK) Equality Act 2010.
<br>

<ul>
    <li>Systems that do not use personal data, do not come within the scope of data protection legislation, e.g. prediction of the weather.</li>
    <li>Systems that do use personal data for development, testing and deployment would come within the scope of data protection legislation.</li>
</ul>
<h2>Automated decision-making and Data Protection</h2>


<div>

    <h3>EU General Data Protection Regulations & UK Data Protection Act</h3>
    <ul>
        <li>Regulate the processing and collection of personal data.</li>
        <li>Automated decision-making systems using personal data fall under these regulations.</li>
    </ul>

    <h3>Scope of Data Protection Legislation</h3>
    <strong>1. Systems Not Using Personal Data:</strong>
    <p>If an automated decision-making system, like a weather prediction model, doesn't use personal data, it's outside the scope of data protection legislation.</p>

    <strong>2. Systems Using Personal Data:</strong>
    <p>Systems used in development, testing, and deployment of automated decision-making that utilize personal data fall within the scope of data protection legislation.</p>

    <h3>Article 22 of GDPR</h3>
    <p>States that individuals have the right not to be subject to decisions made solely on automated processing, including profiling, with legal or significant effects.</p>

    <strong>Solely Automated Decision Making:</strong>
    <p>Refers to decisions without human involvement. However, if the decision process includes significant human intervention, it's not considered solely automated.</p>
    <p>Example: The HART tool used by police to assess reoffending risk isn't solely automated as the final decision rests with the officer.</p>

    <h3>Legal Impact and Significant Effects</h3>
    <p>Decisions affecting legal rights or having significant impacts on individuals fall under GDPR.</p>
    <p>Example: Loan approval processes can significantly impact an individual’s life.</p>

    <h3>Circumstances for Permitted Automated Decision-Making</h3>
    <ul>
        <li><strong>Performance of a Contract</strong></li>
        <li><strong>Member State Legislation:</strong> Permitted if the country's legislation allows it.</li>
        <li><strong>Explicit Consent:</strong> The individual must be fully informed and consent to the automated decision-making process.</li>
    </ul>

    <h3>Data Protection Impact Assessment (DPIA)</h3>
    <p>Required for high-risk data processing and for processes that fall under Article 22.</p>
    <p>Helps to identify and mitigate risks in profiling and automated decision-making processes.</p>

    <h3>Rights Under GDPR</h3>
    <ul>
        <li><strong>Article 15:</strong> Right to be informed about automated decision-making processes.</li>
        <li><strong>Article 21:</strong> Right to object to processing personal data, including profiling.</li>
        <li><strong>Article 22:</strong> Right to not be subject to solely automated decision-making processes.</li>
    </ul>

    <h3>UK Equality Act 2010</h3>
    <p>Automated decision-making systems must not result in discrimination based on protected characteristics.</p>
    <p>Organizations must demonstrate non-discrimination and communicate this effectively.</p>

    <h3>Conclusion</h3>
    <p>Automated decision-making systems must navigate the complexities of data protection and equality laws, ensuring non-discriminatory practices and compliance with GDPR and the UK Data Protection Act.</p>
</div>






<ul>

    <ul>
        <li>Development</li>
        <li>Testing</li>
        <li>Deployment</li>
    </ul></li>
</ul>Data protection legislation is neutral, there is no implementation or technology referenced.
<br>
<br>
The (EU/UK) GDPR has provisions for:
<br>
<ul>
    <li>profiling</li>
    <li>automated decision-making</li>
</ul>
<h3>Article 22</h3>

The GDPR restricts the use of solely automated decision-making that has legal impact or similar significant effect on an individual.
<br>

<h3>Solely Automated Processes</h3>

Solely means the decision making process has no human involvement and is completely automated.
<br>

<ul>
    <li>The process is still considered solely automated if humans are involved in the entry of data.</li>

    <ul>
        <li>The process would not be considered solely automated, if the outcome is considered by a human being as part of the decision process.</li>

    </ul>
    <li>Human inclusion must be significant to avoid being deemed solely automated.</li>

    <ul>
        <li>A human must have the potential to consider and alter the decision before application, rather than just blindly approving automated decisions.</li>

        <ul>
            <li>An automated decision having legal impact is a decision that affects the legal rights of an individual.</li>
            <li>A decision that has similar significant effect that impacts on the individual.</li>

        </ul>

    </ul>
    <li>Automated-decision making is permitted under specific circumstances.</li>
</ul>
<h3>Data Protection Impact Assessment</h3>

<ul>
    <li>Article 35 (EU) GDPR requires organisations to perform a Data Protection Impact Assessment (DPIA) when processing of data is likely to be high risk to the rights of the individual.</li>
    <li>Article 35(3)a states a DPIA is required if you meet the requirements of Article 22.</li>

    <ul>
        <li>DPIA can also be used to determine if processing does come under Article 22.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Determine the risks of the profiling and automated decision-making process and the mitigation to those risks.</li>
    <li>If in scope of Article 22 of the GDPR, organisations must:</li>

    <ul>
        <li>develop processes that allow individuals to exercise rights</li>
        <li>put safeguards in place</li>
        <li>inform the individual about the use of automated decision-making process and profiling.</li>
    </ul></li>
</ul>
<h3>Relevant rights</h3>
<ul>
    <li>Right to be informed</li>
    <li>Article 15 (EU) GDPR gives the right for individuals to be informed about:</li>

    <ul>
        <li>existence of systems that use solely automated decision-making that have legal impacts or similar significant effects.</li>
        <li>information on the process involved</li>
        <li>perceived consequences of decisions made for individuals.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Right to object</li>
    <li>Article 21 (EU) GDPR gives individuals the right to object to the processing of their personal data, including profiling, in specific scenarios.</li>
    <li>Absolute right to object profiling for marketing purposes.</li>
</ul>

<ul><li>Individuals rights</li>
    <li>Article 22 (EU) GDPR empowers individuals with the right not to be subjected to solely automated decision-making that have legal impacts or similar significance effects.</li>
    <li>Exceptions:</li>

    <ul>
        <li>adopt safeguards, including human intervention</li>
        <li>contest the decision</li>
        <li>express concerns or view.</li>
    </ul></li>
</ul>
<h2>Equality</h2>

The (UK) Equality Act 2010 outlines a number of protected characteristics.
<br>
<ul>
    <li>Automated decision-making must not result in discrimination that:</li>

    <ul>
        <li>produces decisions that result in individuals being treated different due to a protected characteristic</li>
        <li>impact of the decision is different for an individual with a protected characteristic than to those without.</li>
        <li>Organisations must communicate effectively that decisions do not discriminate on any of the protected characteristics.</li>
    </ul></li>
</ul>
<h1>Platforms and Profit</h1>
<h2> Overview</h2>
The Platforms & Profit theme considers the following material:
<br>

<ul>
    <li>Intellectual Property Rights</li>
    <li>Copyright</li>
    <li>Confidentiality</li>
    <li>Patents</li>
    <li>Trade marks</li>
    <li>Trade secrets</li>
    <li>Reverse Engineering</li>
    <li>Software locks</li>
    <li>Software licenses</li>
</ul>
<h2>IP Rights</h2>

<div>
    <p>Intellectual property (IP) differs from tangible property, where IP theft results in the loss of control over the property, not the property itself.</p>

    <h3>Examples of Intellectual Property:</h3>
    <ul>
        <li><strong>Music Composition:</strong> IP theft occurs when someone replicates your music without permission, leading to loss of control over its use.</li>
        <li><strong>Software Development:</strong> Copying software code and using it elsewhere constitutes IP theft, affecting control over its distribution and use.</li>
    </ul>

    <h3>Types of Intellectual Property Rights:</h3>
    <p>IP rights often involve preventing others from performing certain activities.</p>
    <ul>
        <li><strong>Registered Rights:</strong> Include patents and trademarks, recognized by legal authorities.</li>
        <li><strong>Unregistered Rights:</strong> Like copyright, arise naturally without formal registration.</li>
    </ul>

    <h3>Public Domain in IP:</h3>
    <p>Refers to works not protected by IP rights, either because the rights have expired or were waived.</p>

    <h3>Forms of Intellectual Property Rights:</h3>
    <ul>
        <li><strong>Copyright:</strong> The right to copy documents, images, audio, video.</li>
        <li><strong>Obligation of Confidence:</strong> Involves confidentiality in employer-employee relationships.</li>
        <li><strong>Patents:</strong> Temporary monopolies on inventions to encourage innovation and public disclosure.</li>
        <li><strong>Trademarks:</strong> Used to identify specific products or services.</li>
        <li><strong>Software as IP:</strong> Recognized due to the investment in development, requiring laws to protect it.</li>
    </ul>

    <h3>Global Aspect:</h3>
    <p>IP rights need international recognition and protection, with many countries adhering to agreements like the Paris Convention and the Berne Convention.</p>
</div>

<ul>
    <li>Tangible vs intellectual property</li>

    <ul>
        <li>You have parked your bike on the campus. Someone else takes it away.</li>
        <li>You have been deprived of your bike.</li>
        <li>This is theft of tangible property.</li>
    </ul></ul>

<ul><ul>	<li>You have composed a song, writing down the score and lyrics in your notebook. Someone else looks at the notebook, memorises or copies the song, and publishes it.</li>
    <li>You still have the song, but you have been deprived of the right to use or sell it as your own.</li>
    <li>You have written a computer program. Someone else copies the program, and uses or sells it.</li>
    <li>You still have the program, but you have been deprived of the right to use or sell it as your own.</li>
    <li>This is theft of intellectual property.</li>
</ul></ul>

<ul><ul></ul></li>
</ul><h3>Intellectual Property</h3>
<ul>
    <li>Thefts of IP are covered by special laws, known as intellectual property rights (IPR).</li>
    <li>IP is particularly easy to take across national boundaries, so international laws are essential.</li>
</ul>

<ul><li>This was recognised long ago:</li>

    <ul>
        <li>Paris Convention 1883, covering trade marks and patents</li>
        <li>Berne Convention 1886, covering copyright.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Most (but not all) countries have signed these conventions.</li>
</ul>
<h2>Intellectual Property Rights</h2>

<h3>Intellectual Property Rights</h3>

Intellectual Property Rights are negative rights as they demand someone does not perform an activity.
<br>

<ul>
    <li>Registered IPRs are approved or granted by states, typically by some official entity.</li>

    <ul>
        <li>patents</li>
        <li>trademarks</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Unregistered IPRs come into existence and are not approved or granted by states.</li>

    <ul>
        <li>Copyright</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Public domain, in the context of IPR, essentially means no IPR exists.</li>
</ul>
<h3>Types of IP</h3>
<ul>
    <li>Copyright:</li>

    <ul>
        <li>the right to copy documents, images, audio/video recordings, programs.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Obligation of confidence:</li>

    <ul>
        <li>protection for confidential information received but not intended to be passed on to others.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Patent:</li>

    <ul>
        <li>a temporary monopoly on exploiting an invention.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Trade mark:</li>

    <ul>
        <li>a sign intended to identify a particular product</li>
    </ul></li>
</ul>
<h3>Software as IP</h3>
<ul>
    <li>Software systems are expensive to develop, so software is valuable IP.</li>
</ul>

<ul><li>Software has characteristics different from older forms of IP (documents, images, audio/video recordings, inventions, trade marks).</li>
</ul>

<ul><li>So existing IP laws had to be extended in the 1980s and 1990s.</li>
</ul>
<h2>Copyright</h2>
<h3>Overview</h3>
<ul>
    <li>In UK, the primary law is the Copyright, Design and Patents Act 1988 (CDPA).</li>
</ul>

<ul><li>CDPA protects:</li>

    <ul>
        <li>original literary, dramatic, musical, and artistic works</li>
        <li>sound recordings, films, TV programmes.</li>

    </ul>
    <li>Copyright lasts up to 70 years after the author's death.</li>
</ul>

<ul><li>CDPA classifies software as a literary work! ' Only original code is protected.</li>

    <ul>
        <li>Boilerplate code is not protected.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>An unregistered right that comes into effect upon the creation of original work.</li>
</ul>

<ul><li>The scope of copyright is typically restricted to the expression of an idea, not so much the idea itself.</li>
</ul>

<ul><li>At the turn of the century scope was expanded to permit legal action against interference with measures designed to protect copyright.</li>
</ul>
<h3>Copyright Ownership</h3>
<ul>
    <li>The author owns the copyright, unless the author did the work for his/her employer.</li>

    <ul>
        <li>If the author is an independent contractor, he/she owns the copyright unless the contract says otherwise.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>The copyright owner has exclusive rights:</li>

    <ul>
        <li>to make copies of the work (including download of a web page)</li>
        <li>to sell, rent, lend, or give away copies of the work</li>
        <li>to adapt the work (including translation to a different language).</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Others must seek permission from the copyright owner.</li>

    <ul>
        <li>In some cases permission is implicit. (E.g., downloading a web page is permitted, but not storing it permanently.)</li>
    </ul></li>
</ul>
<h3>Software Copyright</h3>
<ul>
    <li>If you are authorised to use a program, CDPA permits:</li>

    <ul>
        <li>making one backup copy of the program</li>
        <li>decompiling the program to fix bugs</li>
        <li>decompiling the program to understand how to write another program to inter-operate with it.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Databases are a special case:</li>

    <ul>
        <li>protected if the contents are the author's own intellectual creation</li>
        <li>protected (but only for 15 years) if the contents are purely factual but their collection required substantial investment.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Copyright infringement is:</li>

    <ul>
        <li>a civil offence if done for individual reasons</li>
        <li>a criminal offence if done for commercial reasons.</li>
    </ul></li>
</ul>
<h2>Confidentiality</h2>
<h3>Confidentiality</h3>
If someone reveals information obtained under an obligation of confidence, he/she can be sued in a civil court.
<br>
<br>
Examples:
<br>
<ul>
    <li>Employees must not reveal confidential information about their employer's business.</li>
</ul>

<ul><li>When a company engages a contractor or consultant, the contract will normally include a confidentiality clause.</li>
</ul>

<ul><li>When two companies are discussing possible collaboration, they will sign a non-disclosure agreement to protect exchanged information.</li>
</ul>
<h3>Public interest disclosure</h3>

<ul>
    <li>Sometimes the "public interest" overrides an obligation of confidence. This makes it possible for employees (whistle-blowers) to expose wrong-doing.</li>
</ul>

<ul><li>The UK Public Interest Disclosure Act 1998 (PIDA) provides protection for whistle-blowers who expose:</li>

    <ul>
        <li>criminal offences</li>
        <li>failure to comply with legal obligations ' danger to health and safety</li>
        <li>environmental damage</li>
        <li>concealment of the above.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>The whistle-blower must first approach the employer, then a professional body or public official (not the media!).</li>
</ul>
<h2>Patents</h2>
A patent is a temporary right for an inventor to prevent others from exploiting his/her invention without permission.
<br>
<br>
The inventor must apply for a patent to a national patenting office.
<br>
<br>
If the patent is granted, the details of the invention will be published.
<br>
This will enable anyone to exploit the invention after the patent has expired.
<br>
<ul>
    <li>In UK, CDPA covers patents. It allows a patent to be granted only if the invention is:</li>

    <ul>
        <li>new (not previously disclosed or used publicly) ' inventive (non-obvious)</li>
        <li>capable of industrial application</li>
        <li>not in an excluded area.</li>
    </ul></li>
</ul>
The European Patent Convention lists excluded areas:
<br>
<ul>

    <ul>
        <li>anything covered by copyright</li>
        <li>scientific theories</li>
        <li>mathematical methods</li>
        <li>methods and programs.</li>
    </ul></li>
</ul>
<h3>Software Patents</h3>
Nevertheless, the European Patent Office has granted software patents. And individual countries have a variety of policies. Result: confusion!
<br>

Arguments for and against software patents:
<br>
<ul>
    <li>It is illogical to grant a patent for a hardware device but deny it for a device that uses software to do the same thing.</li>
    <li>Patents reward research and development, and encourage investment in novel technologies.</li>
    <li>Much software development is done by small companies, who cannot afford to defend patents in court.</li>
    <li>In practice, software development has not been inhibited by lack of patents.</li>
</ul>
<h2>Trademarks</h2>
<h3>Overview</h3>
The UK Trade Marks Act 1994 provides protection for trade marks.
<br>
<br>
It defines a trade mark as any sign capable of being represented graphically which is capable of distinguishing goods or services of one undertaking from those of other undertakings. A trade mark may, in particular, consist of words (including personal names), designs, letters, numerals or the shape of goods or their packaging.
<br>
<br>
Trade marks can be registered with the UK Intellectual Property Office, or its equivalent in other countries.
<br>
Selling anything with an unauthorised trade mark is a criminal offence in the countries where it is registered.
<br>

<h3>Software trade marks</h3>
Software is now usually distributed without physical packaging.
<br>
<br>
To protect the producer, the software should display the trade mark prominently whenever it runs.
<br>
<br>
Selling pirated software that displays the genuine producer's trade mark is then a criminal offence.
<br>

<h2>Trade Secrets</h2>
Trade secrets can be protected indefinitely, as long as they are kept secret.
<br>
<br>
If such secrets become public, securing a patent is unlikely.
<br>
<br>
Owners of trade secrets can take legal action
<br>
<ul>
    <li>against those that misappropriate their secrets.</li>
    <li>against those that utilise such secrets, e.g. a third-party.</li>
</ul>
Trade secrets are typically protected under general tort law.
<br>
<ul>

    <ul>
        <li>valuable information that is not widely known.</li>
        <li>valuable information that delivers benefit to the holder, because it has been kept secret.</li>
    </ul></li>
</ul>Empowering those that have been a reasonable attempt to keep their secrets against those that have utilised, obtained and/or revealed without authorisation.
<br>
<br>
In the US, there has been increasing legal harmonisation around trade secrets across the country.
<br>
The (US) Economic Espionage Act (1996) is designed to counter trade secret theft.
<br>
<br>
In European law, that has also been increasing legal harmonisation around trade secrets across member states.
<br>
Directive 2016/943 on the protection of undisclosed know-how and business information (trade secrets) against their unlawful acquisition, use and disclosure.
<br>
Trade secret holders can use a range of tactics to kept information secret, including technology and legal tools.
<br>
<ul>
    <li>Legal tools include the use of non-disclosure agreements (though such a tool may not be that effective).</li>
</ul>The motivation for favouring trade secrets over patents is that companies can exploit their innovation for an indefinite period of time.
<br>
<br>
Risk being able to prevent others from exploiting the innovation through reverse engineering.
<br>

<h2>Reverse Engineering</h2>
<h3>Overview</h3>
<ul>
    <li>Reverse engineering can be considered the lawful approach to obtain trade secrets.</li>

    <ul>
        <li>promotes the use of patents if trade secret owners want to secure income.</li>
        <li>promotes economic activity through competition and innovation without impacting significantly on the creator.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Misappropriation of trade secrets, using tactics such as bribery or espionage, is not fair game.</li>
</ul>

<ul><li>Scientific investigation on something purchased publicly to reveal its secrets is generally perceived as acceptable.</li>
</ul>

<ul><li>Reverse engineering is typically perceived as an accepted practice in IPR, but does differ depending on the form.</li>
</ul>

<ul><li>In European law, software owners are typically permitted to decompile programs to understand and support interoperability.</li>
</ul>

<ul><li>In United States law, similar software owners are typically permitted to decompile programs to understand and support interoperability.</li>

    <ul>
        <li>permitted 'tinkering' also includes fixing bugs, adapting for a platform etc.</li>
    </ul></li>
</ul>
<h3>Software</h3>
In European law, software owners are typically permitted to decompile programs to understand and support interoperability.
<br>

In United States law, similar software owners are typically permitted to decompile programs to understand and support interoperability.
<br>
<ul>
    <li>permitted 'tinkering' also includes fixing bugs, adapting for a platform etc.</li>
</ul>
<h3>First Sale</h3>
<ul>
    <li>The first sale rule can be considered the effective protection of individuals to reverse engineer products.</li>

    <ul>
        <li>individuals that have acquired products have the right to modify, use and resell those items.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>IP owners can control the first sale, but beyond that IP owners generally lose control of subsequent sales of products.</li>
</ul>

<ul><li>The first rule has many positive outcomes:</li>

    <ul>
        <li>Preservation of products.</li>
        <li>Secondary market.</li>
        <li>Transaction clarity.</li>
        <li>Product innovation.</li>
    </ul></li>
</ul>
<h3>An Example - Case Genie</h3>

Case study: Game Genie
<br>
<br>
Video game cheat cartridge originally created by Codemasters and sold by other companies.
<br>
<br>
The cartridge made small alterations to video games to support players, e.g. infinite lives.
<br>
<br>
Nintendo stated that the Game Genie infringed derivative work rights.
<br>
<ul>
    <li>cartridge afforded consumers the ability to produce unauthorised derivative works of Nintendo games.</li>
</ul>
Court debated whether the cartridge actually produced derivative works as it only interfered or substituted signals from the game cartridge to the system with its own and only under direction of the individual.
<br>
<br>
Court also felt the number of alterations was small and could be deemed fair use.
<br>
<br>
Court argued the cartridge did not impact the market for Nintendo games as it only be used in conjunction with the games.
<br>
<ul>
    <li>The cartridge itself was also had no value without such games.</li>
</ul>
<h2>Software Locks</h2>
<h3>Overview</h3>

The use of software locks or activation process to enable the use of software is not unusual, e.g. license key entry.
<br>
<br>
Neither is the termination of access or reduce access when individuals to no pay usage fees, e.g. software as a service.
<br>
<br>
Undisclosed, subsequent and time-based software locks are far more problematic.
<br>
<ul>
    <li>preventing use of software after a period without prior notification to the user.</li>
    <li>such attempts have been successfully considered as attacks against cyber systems and space.</li>
</ul>
<h3>Reverse engineering anti-circumvention solutions</h3>
<ul>
    <li>Reverse engineering technological solutions designed to protect IP is very problematic.</li>
</ul>

<ul><li>Copyright law has generally evolved to dissuade interference with solutions to protect IP.</li>
</ul>

<ul><li>In UK law, some exceptions do exist in the CPDA.</li>

    <ul>
        <li>exemptions exist for those researching cryptography except on computing programs.</li>
        <li>researchers must be careful as exemptions do not come into effect if their actions (performance or dissemination) negatively affect the copyright owner.</li>
        <li>no exemptions exist for computer programs.</li>
    </ul></li>
</ul>
<h3>Kerchoffs' Principle</h3>
Every aspect of a cryptosystem should be secure, even if every aspect is known, except the key.
<br>

<h3> Case study: Volkswagen v Garcia</h3>
<ul>
    <li>engine immobiliser and keys utilise specialised cryptographic hardware, produced under license.</li>
</ul>

<ul><li>researchers reversed engineered the algorithm and wanted to publish results.</li>
    <li>researchers could have purchased car and reversed engineered algorithm using chip-slicing.</li>
    <li>researchers instead reversed a third-party software that was used by mechanics for diagnostic purposes.</li>
    <li>individuals keen to maintain the trade secret status of their work, sought to prevent researchers from disseminating their work.</li>
</ul>

<ul><li>Court agreed that researchers could have reversed engineered the algorithm via chip-slicing, this would be fair game.</li>
    <li>Court stated that instead the researchers had actually reversed engineered a software product ' that was likely itself only created through improper use of trade secrets.</li>
    <li>Researchers probably knew or should have known that such a product only existed due to such improper use.</li>
    <li>Researchers resolved the issue with the IPR owners and eventually disseminated efforts with work essentially aspects redacted.</li>
</ul>

<h2>Software Licences</h2>
<h3>Overview</h3>
Buying software is not the same as buying something tangible.
<br>
<br>
It usually means buying a copy of the software with a licence to use it, subject to terms and conditions:
<br>
<ul>
    <li>in particular, restrictions on making further copies of the software.</li>
</ul>

<h3>Retail software licences</h3>
<ul>
    <li>Retail software is intended for a mass market.</li>

    <ul>
        <li>The price is modest (typically up to '500).</li>
        <li>The buyer receives one copy of the software with a licence to install it on one computer.</li>
        <li>The licence does not cover maintenance or upgrades.</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>An organisation might prefer to negotiate a bulk licence.</li>

    <ul>
        <li>The price is correspondingly higher.</li>
        <li>The organisation is entitled to install the software on multiple computers.</li>
        <li>The licence might limit the number of installations, or the number of simultaneous users.</li>
    </ul></li>
</ul>
<h3>Corporate software licences</h3>
<ul>
    <li>Corporate software is intended for large organisations. The market might be:</li>

    <ul>
        <li>thousands for generic software (e.g., customer relation management, human resource management, computer-aided design)</li>
        <li>hundreds for more specialised software</li>
        <li>tens for highly specialised software</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Licence characteristics:</li>

    <ul>
        <li>hefty up-front licence fee (typically '10,000 ' '1,000,000)</li>
        <li>annual maintenance fee (typically 20% of licence fee, or on a scale dependent on volume of usage)</li>
        <li>installation, configuration, upgrades, and support are covered.</li>
    </ul></li>
</ul>
<h3>Open Source Licences</h3>
<ul>
    <li>Open-source software is distributed as source code.</li>

    <ul>
        <li>The price is small (or zero).</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>Typical licence conditions:</li>

    <ul>
        <li>The authors' names and copyright statement must be retained in the source code.</li>
        <li>The source code (perhaps modified) can be reused.</li>
        <li>The source code can be re-distributed, but only under identical licence conditions. (In particular, software obtained free cannot be re-sold for profit.)</li>
    </ul></ul>

<ul><ul>
</ul>
    <li>The Open Software Initiative (OSI) was launched in 1998 to promote software licenced in this way</li>
</ul>
<h3>Free Software</h3>
Free software is distributed completely free of charge.
<br>
<br>
The Free Software Foundation was launched by Richard Stallman in 1985 to support the GNU project:
<br>
<ul>
    <li>a version of Unix</li>
    <li>a suite of compilers for C, C++, Pascal, Ada, Java, etc. (all using common code generators, and targetable to a variety of platforms)</li>
    <li>libraries</li>
    <li>utilities.</li>
</ul>
Linux, developed by Linus Torvalds in 1991, is the superstar of free software and runs on more platforms than any other OS.<br>

<h1>Society & Surveillance</h1>
<h2>Overview</h2>

<p>The Society & Surveillance theme considers the following material: </p>

<ul>
    <li>Privacy</li>
    <li>Data protection</li>
    <li>General Data Protection Regulations</li>
    <li>Mointoring of communications</li>
    <li>Freedom of information</li>
    <li>Information Security</li>
    <li>Internet</li>
    <li>Defamation</li>
    <li>Pornography</li>
    <li>Spam</li>
</ul>

<p>Privacy</p>

<ul>
    <li>"The right of people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated, and no Warrants shall issue, but upon probable cause, supported by Oath or affirmation, and particularly describing the place to be searches, and the persons or things to be seized."</li>
    <ul>
        <li>US Constitution, Amendment IV via the Bill of Rights (1791).</li>
    </ul>
</ul>

<ul>
    <li>"No one shall be subjected to arbitrary interference with his privacy, family, home or correspondence, nor to attacks upon his honour or reputation. Everyone has the right of the protection of the law against such interference or attacks."</li>
    <ul>
        <li>UN, Universal Declaration of Human Rights, Article 12 (1948).</li>
    </ul>
</ul>

<ul>
    <li>"Everyone has the right to respect for his or her private and family life, home and communications."</li>
    <ul>
        <li>EU, Charter of Fundamental Rights, Article 7 (2000).</li>
    </ul>
</ul>

<ul>
    <li>UK has no comprehensive privacy law.</li>
    <li>The Data Protection Act 1998 covers stored data, but not communications.</li>
    <li>The Regulation of Investigatory Powers Act 2000 regulates monitoring of postal, phone, and computer communications.</li>
</ul>

<h2>Data Protection Legislation</h2>

<p>EU has sought to harmonise national data protection laws.</p>

<ul>
    <li>European Directive on Data Protection 1995</li>
    <li>General Data Protection Regulation (GDPR) 2018</li>
</ul>

<ul>
    <li>USA has no comprehensive data protection law, but a patchwork of state laws and regulations.</li>
</ul>

<ul>
    <li>Many other countries have weak data protection laws or none at all.</li>
    <li>Much potential for conflict:</li>
    <ul>
        <li>where data is sent from one country to another</li>
        <li>where data is held by multinationals (e.g., Google, Facebook).</li>
    </ul>
</ul>

<ul>
    <li>Data Protection Act 1984 protected individuals from misuse of data by large organisations:</li>
    <ul>
        <li>use of inaccurate/incomplete/irrelevant personal data</li>
        <li>use of personal data by unauthorised persons</li>
        <li>use of personal data for purposes other than those for which it was collected.</li>
    </ul>
</ul>

<ul>
    <li>Data Protection Act 1998 (DPA)</li>
    <ul>
        <li>conforms to European Directive on Data Protection 1995</li>
        <li>covers Internet data as well as stored data</li>
        <li>no longer assumes that large organisations are the only possible offenders.</li>
    </ul>
</ul>

<h3>Data Protection Act </h3>

<h4>Terminology</h4>

<ul>
    <li>Data: information that is processed or collected.</li>
</ul>

<ul>
    <li>Personal data: data that relates to a living person who can be identified.</li>
</ul>

<ul>
    <li>Data subject: the person that the data refers to.</li>
</ul>

<ul>
    <li>Data controller: a person within an organisation who determines how or why personal data is processed.</li>
</ul>

<ul>
    <li>Processing: obtaining, recording, or holding information or data, or carrying out operations on it.</li>
</ul>

<ul>
    <li>Information Commissioner: government-appointed official responsible for enforcing the DPA.</li>
</ul>

<ul>
    <li>Sensitive personal data:</li>
    <ul>
        <li>racial/ethnic group</li>
        <li>political/religious views</li>
        <li>physical/mental health</li>
        <li>sexual orientation</li>
        <li>criminal record (including allegations)</li>
        <li>genetic information.</li>
    </ul>
</ul>

<ul>
    <li>Sensitive personal data is subject to stricter rules for processing.</li>
</ul>

<h4>Principles </h4>

<ul>
    <li>"Personal data shall be processed fairly and lawfully and in particular shall not be processed unless (a) [the data subject grants consent], and (b) in the case of sensitive personal data, [the data subject grants explicit consent]."</li>
    <li>"Personal data shall be obtained only for one or more specified and lawful purposes, and shall not be further processed in any manner incompatible with that purpose or those purposes." E.g., individuals� medical records may not be used for research (even if anonymised), except with their consent.</li>
    <li>"Personal data shall be adequate, relevant and not excessive in relation to the purpose or purposes for which they are processed."</li>
    <li>"Personal data shall be accurate and, where necessary, kept up to date."</li>
    <li>"Personal data processed for any purpose or purposes shall not be kept for longer than is necessary for that purpose or those purposes." E.g., financial data must be kept for 7 years, for auditing and taxation.</li>
    <li>"Personal data shall be processed in accordance with the rights of data subjects under this Act." Data subjects are entitled to be told what data is held about them, why it is held, which other organisations the data may be disclosed to, etc.</li>
    <li>"Appropriate technical and organisational measures shall be taken against unauthorised or unlawful processing of personal data and against accidental loss or destruction of, or damage to, personal data." The organisation must keep the personal data secure (integrity checks, access controls, backups, employee vetting).</li>
    <li>"Personal data shall not be transferred to a country or territory outside the European Economic Area unless that country or territory ensures an adequate level of protection for the rights and freedoms of data subjects in relation to the processing of personal data."</li>
</ul>

<h3>Data Protection Act 2018</h3>
<p>The General Data Protection Regulations (GDPR) permits member states to supplement or deviate from it with national laws.</p>
<p>Data Protection Act 2018 updates data protection laws in the United Kingdom to supplement the GDPR.</p>
<ul>
    <li>covering aspects beyond the scope of the GDPR, such as national security.</li>
    <li>permitted deviations, e.g. lower age of consent.</li>
</ul>

<p>Data Protection Act 2018 does not transpose GDPR into UK law, for the purposes of leaving the European Union this is achieved by other legislation.</p>

<h2>General Data Protection Regulations</h2>

<ul>
    <li>The GDPR restricts the use of solely automated decision-making that has legal impact or similar significant effect on an individual.</li>
    <li>Solely means the decision making process has no human involvement and is completely automated.</li>
    <li>The process is still considered solely automated if humans are involved in the entry of data.</li>
    <li>The process would not be considered solely automated, if the outcome is considered by a human being as part of the decision process.</li>
    <li>Human inclusion must be significant to avoid being deemed solely automated.</li>
    <li>A human must have the potential to consider and alter the decision before application, rather than just blindly approving automated decisions.</li>
    <li>An automated decision having legal impact is a decision that affects the legal rights of an individual.</li>
    <li>A decision that has similar significant effect that impacts on the individual.</li>
    <li>Automated-decision making is permitted under specific circumstances.</li>
    <li>Organisations also have additional responsibilities when utilising automated decision-making.</li>
    <li>Organisations must:</li>
    <ul>
        <li>develop processes that allow individuals to exercise rights</li>
        <li>put safeguards in place</li>
        <li>inform the individual about the use of automated decision-making process and profiling.</li>
    </ul>
</ul>

<h2>Monitoring of communications</h2>

<ul>
    <li>In this context, communications include:</li>
    <ul>
        <li>letters</li>
        <li>phone calls</li>
        <li>text messages</li>
        <li>e-mail messages</li>
        <li>anything posted on a blog or social network</li>
    </ul>
</ul>

<ul>
    <li>Communications data are defined as:</li>
    <ul>
        <li>identities of senders and receivers</li>
        <li>their locations</li>
        <li>dates and times</li>
        <li>sizes or durations</li>
        <li>but not the content of communications</li>
    </ul>
</ul>

<h3>UK Regulation of Investigatory Powers Act (RIPA):</h3>
<p>Regulation of Investigatory Powers Act 2000 (RIPA) created a framework for controlling lawful interception of computer/phone/postal communications:</p>
<ul>
    <li>*by specified public agencies (police, intelligence, tax collection), but only for preventing or detecting serious crime</li>
    <li>*by communication service providers (CSPs) and employers, but only for specified legitimate reasons</li>
</ul>

<h3>RIPA: monitoring by service providers</h3>
<p>RIPA permits any organisation providing computer/phone services (e.g. an ISP or employer) to monitor and record users' communications, for legitimate reasons</p>
<p>Legitimate reasons include:</p>
<ul>
    <li>*ensuring compliance with the organisation's regulations and procedures</li>
    <li>*upholding standards (e.g. calls monitored for training purposes)</li>
    <li>*preventing or detecting crime (e.g. theft, fraud)</li>
    <li>*investigating/detecting unauthorised use of communication systems</li>
</ul>

<ul>
    <li>Users must be told that such interceptions may take place</li>
</ul>

<h3>RIPA: monitoring by public agencies</h3>
<ul>
    <li>RIPA permits specified public agencies to intercept communications:</li>
    <ul>
        <li>*only for specified purposes, such as preventing/detecting serious crime</li>
        <li>*only with a warrant granted by a senior official, identifying the suspect persons or organisations</li>
    </ul>
</ul>

<ul>
    <li>Originally only police, intelligence, and tax agencies could seek warrants</li>
    <li>Later extended to numerous other public agencies</li>
</ul>
<h3>RIPA: controversy and ruling</h3>
<ul>
    <li>RIPA has been criticised by some MPs and campaign groups for excessive intrusion</li>
    <li>RIPA has been criticised by intelligence agencies for not going far enough!</li>
</ul>

<ul>
    <li>Blanket surveillance was rules illegal by the EU Court of Justice in 2014:</li>
    <ul>
        <li>*"The court takes the view that, by requiring the retention of those [communications] data and by allowing the competent national authorities to access those data, the directive interferes in a particularly serious manner with the fundamental rights to respect for private life and to the protection of personal data. [...]"</li>
    </ul>
</ul>

<h3>UK Investigatory Powers Act (IPA) (2016):</h3>
<ul>
    <li>The Investigatory Powers Act 2016 reforms interception of communication:</li>
    <ul>
        <li>*Brought together all the powers available to intelligent and security services to obtain communications</li>
        <li>*Refines how these powers are monitored and authorised</li>
        <li>*Ensures Internet connection records can be retained for law enforcement purposes</li>
    </ul>
</ul>

<h2>Freedom of information</h2>
<p>The basic principle of freedom of information is that information held by public bodies should be available to the public (with certain exceptions). </p>

<p>ey questions:</p>
<ul>
    <li>Which public bodies should be covered?</li>
    <li>Which types of information should be covered? Which should be treated as exceptions?</li>
</ul>

<h3>The Freedom of Information Act 2000 (FoIA) </h3>
<ul>
    <li>FoIA aimed:</li>
    <ul>
        <li>to provide clear rights of access to information held by bodies in the public sector</li>
        <li>to enable anyone to apply for access to such information</li>
    </ul>
</ul>

<ul>
    <li>FoIA's answers to the key questions:</li>
    <ul>
        <li>Public bodies include</li>
        <ul>
            <li>Parliament,</li>
            <li>all government departments,</li>
            <li>local authorities,</li>
            <li>NHS,</li>
            <li>universities,</li>
            <li>schools,</li>
            <li>etc.</li>
        </ul>
        <li>Information covered includes</li>
        <ul>
            <li>printed documents,</li>
            <li>electronic documents,</li>
            <li>e-mails,</li>
            <li>etc.</li>
            <li>(Exceptions include classified documents)</li>
        </ul>
    </ul>
</ul>

<p>There's a potential conflict between FoIA and DPA. Responding to FoIA requests can be extremely costly.</p>

<h2>Information security</h2>

<ul>
    <li>Information security is concerned with the protection of all information assets. We want to maintain the confidentiality, integrity and availability of data. Information is vulnerable from physical threats as well as cyber threats - information security should mitigate this. CIA Triad (ISO/IEC27000):</li>
    <li>confidentiality is the property that information is not made available or disclosed to unauthorised individuals, entities or processes</li>
    <ul>
        <li>integrity is the property of accuracy and completeness</li>
        <li>availability is the property of being accessible and usable upon demand by an authorised entity</li>
        <li>not sufficient to maintain just one of these principles - must maintain all!</li>
    </ul>
</ul>

<ul>
    <li>Directive 2013/40/EU on attacks against information systems established minimum rules and regulations across member states towards attacks on information systems.</li>
</ul>

<h3>Obfuscation-based inference controls</h3>

<ul>
    <li>Anonymisation</li>
    <ul>
        <li>decoupling identification from information</li>
        <li>achieving anonymisation is extremely difficult.</li>
        <li>labelling a data set as anonymised remains unclear.</li>
    </ul>
</ul>

<ul>
    <li>Generalisation</li>
    <ul>
        <li>reducing the precision of data to reduce the likelihood of informing identity.</li>
        <li>example: use of ranges</li>
    </ul>
</ul>

<ul>
    <li>Suppression</li>
    <ul>
        <li>suppression information, making fewer data available to attacker to reduce inferences</li>
        <li>example: suppression gender in 50% of data records.</li>
    </ul>
</ul>

<ul>
    <li>Dummy addition</li>
    <ul>
        <li>adding dummies or fake data points to data sets prior to providing to other party</li>
        <li>example: artificial records.</li>
    </ul>
</ul>

<ul>
    <li>Perturbation</li>
    <ul>
        <li>adding noise to data to reduce the ability of other party to form inferences.</li>
        <li>example: differential privacy</li>
    </ul>
</ul>

<h2>Internet </h2>
<ul>
    <li>The Internet�s benefits are well-known:</li>
    <ul>
        <li>easy access to data of all types (text, images, video, audio)</li>
        <li>easy communication</li>
        <li>transcends national boundaries.</li>
    </ul>
</ul>

<ul>
    <li>But it also brings risks:</li>
    <ul>
        <li>some data might be harmful, even unlawful</li>
        <li>communication can be abused (e.g., by criminals and "trolls")</li>
        <li>regulation is difficult (because it transcends boundaries)!</li>
    </ul>
</ul>

<h3>Internet Service Providers</h3>
<p>An internet service provider (ISP) provides access to the Internet for its customers (organisations and individuals).</p>
<p>Can an ISP be held responsible for unlawful data up/downloaded by its customers?</p>

<ul>
    <li>EU law (Directive 2000/31/EC) distinguishes between:</li>
    <ul>
        <li>mere conduit</li>
        <li>caching</li>
        <li>hosting.</li>
    </ul>
</ul>

<ul>
    <li>Mere conduit:</li>
    <ul>
        <li>The ISP simply transmits data up/downloaded by the customer.</li>
        <li>Then the ISP is not liable for damages or criminal sanctions.</li>
    </ul>
</ul>

<ul>
    <li>Caching:</li>
    <ul>
        <li>The ISP temporarily stores downloaded data, to speed up future downloading of the same data.</li>
        <li>Then the ISP is not liable for damages or criminal sanctions, provided that:</li>
        <ul>
            <li>it promptly removes or blocks access to cached data when informed that the original data had been removed/blocked, or a court/authority has ordered removal/blocking, etc.</li>
        </ul>
    </ul>
</ul>

<ul>
    <li>Hosting:</li>
    <ul>
        <li>The ISP permanently stores data uploaded by its customers.</li>
        <li>Then the ISP is not liable for damages or criminal sanctions, provided that:</li>
        <ul>
            <li>it is unaware that unlawful data has been uploaded</li>
            <li>it promptly removes/blocks unlawful data as soon as it is discovered the customer who uploaded unlawful data was not acting on the ISP�s authority, etc.</li>
        </ul>
    </ul>
</ul>

<p>When an ISP receives a complaint, how does it judge whether data is unlawful? If the complainant is a powerful organisation or individual, the ISP might prefer to play safe. </p>
<p>Example: A freelance journalist uploads an article alleging that an arms company is ignoring an international arms embargo.</p>
<ul>
    <li>The arms company claims that this is defamation, and demands that the hosting ISP remove the article.</li>
    <li>The ISP cannot evaluate the journalist�s detailed evidence.</li>
    <li>The ISP might decide to remove the article, just to avoid time-consuming and expensive legal action.</li>
</ul>

<h3>Internet Watch Foundation (IWF)</h3>
<p>IWF�s remit includes defamatory material, child abuse images, unlawful adult material, etc.</p>

<p>Anyone can report any potentially unlawful content to IWF.</p>
<ul>
    <li>If IWF judges that the content is unlawful, it informs the local police (or Interpol) and the hosting ISP.</li>
    <li>If the hosting ISP is outside UK, IWF asks all its member ISPs in UK to block access to the unlawful content.</li>
</ul>

<p>Example: An arms company submits its claim of defamation to IWF.</p>
<ul>
    <li>IWF experts judge whether the claim is justified or not.</li>
    <li>If the claim is held to be unjustified, the ISP can safely disregard it.</li>
</ul>

<h3>Law In Different Countries</h3>

<ul>
    <li>US law on ISPs is much looser than EU law. Liability for hosting is the same as liability for mere conduit.</li>
    <li>What is unlawful content varies widely from country to country.</li>
    <ul>
        <li>But the Internet transcends national boundaries.</li>
    </ul>
</ul>

<ul>
    <li>In principle, anyone in country A can access data hosted in another country B (even if the data is lawful in B but unlawful in A).</li>
    <li>Authoritarian governments often try to block access to deprecated hosts, but determined users can get round such blocks.</li>
</ul>

<ul>
    <li>Council of Europe has approved a convention that covers:</li>
    <ul>
        <li>child pornography</li>
        <li>criminal copyright infringement</li>
        <li>computer-related fraud</li>
        <li>hacking</li>
        <li>hate material.</li>
    </ul>
    <li>Ratified by 39/51 member countries</li>
    <ul>
        <li>including USA and some other countries outside Europe.</li>
    </ul>
</ul>

<ul>
    <li>But USA opted out of the hate material clause as they felt it a violation of their constitutional right to freedom of speech.</li>
</ul>

<h2>Defamation</h2>

<p>Defamation is communication of a false statement that harms the reputation of an individual, group, or organisation.</p>
<p>Some countries (e.g., England) distinguish between:</p>
<ul>
    <li>slander (spoken defamation)</li>
    <li>libel (written defamation, including e-mail, blogs, tweets, etc.)</li>
</ul>

<p>Other countries (e.g., Scotland) make no such distinction.</p>
<ul>
    <li>In some countries, criticism of government ministers is treated as defamation.</li>
</ul>

<h2>Pornography</h2>
<ul>
    <li>Before the Internet, porn was relatively easy to control:</li>
    <ul>
        <li>photos and videos recorded on film had to be developed, using specialised equipment</li>
        <li>police could raid shops suspected of storing porn.</li>
    </ul>
</ul>

<ul>
    <li>Now, porn is very difficult to control:</li>
    <ul>
        <li>ubiquitous digital cameras</li>
        <li>enormous size of Internet.</li>
    </ul>
</ul>

<ul>
    <li>There is a wide variation in laws:</li>
    <ul>
        <li>In USA, porn is protected by the constitutional right to freedom of speech (!).</li>
        <li>In Europe, adult porn is usually lawful, provided that it is non-violent, consensual, and inaccessible to children.</li>
    </ul>
</ul>

<ul>
    <li>In some other countries, all porn is unlawful (and even nudity or scanty dress).</li>
</ul>

<p>But there is one consensus: In nearly all countries, possessing or communicating child porn is a criminal offence.</p>

<h2>Spam</h2>
<ul>
    <li>Spam is unsolicited e-mail, sent without consent of recipients, and with no attempt to target only recipients likely to be interested.</li>
</ul>

<ul>
    <li>Various purposes:</li>
    <ul>
        <li>sales (dubious medications, treatments, investments, etc.)</li>
        <li>spreading viruses</li>
        <li>fraud or identity theft (attempting to discover private information).</li>
    </ul>
</ul>

<ul>
    <li>In EU law (EC Directive on Privacy and Electronic Communications 2002):</li>
    <ul>
        <li>Unsolicited e-mail can be sent to individuals only with prior consent.</li>
        <li>Unsolicited e-mail must not conceal the sender�s true address.</li>
        <li>A seller may use e-mail addresses obtained in the normal course of business, but must make it easy for recipients to request the seller to stop sending spam.</li>
    </ul>
</ul>

<ul>
    <li>Onus is on spammers to seek prior consent.</li>
    <ul>
        <li>But does not apply to senders outside EU (90% of all spam).</li>
        <li>Also hard to enforce: individuals must take their own action, even against large companies.</li>
    </ul>
</ul>

<ul>
    <li>In US law (CAN-SPAM Act 2003), spam is lawful if:</li>
    <ul>
        <li>The recipient has not asked the sender to stop.</li>
        <li>The unsolicited e-mail contains a valid e-mail address to request that the sender to stop.</li>
    </ul>
</ul>

<ul>
    <li>This is weaker than EU law:</li>
    <ul>
        <li>No prior consent is required.</li>
        <li>Onus is on the recipient to respond, but this inadvertently confirms that the recipient�s e-mail address is still valid.</li>
    </ul>
</ul>

<ul>
    <li>But US law does have teeth:</li>
    <ul>
        <li>ISPs can and do sue spammers for damages.</li>
        <li>Spammers have been imprisoned and fined.</li>
    </ul>
</ul>

<h1>ACM Code of Ethics and Professional Conduct</h1>
<div>
    <h2>Preamble</h2>
    <p>Computing professionals' actions change the world. To act responsibly, they should reflect upon the wider impacts of their work, consistently supporting the public good. The ACM Code of Ethics and Professional Conduct ("the Code") expresses the conscience of the profession.</p>
    <p>The Code is designed to inspire and guide the ethical conduct of all computing professionals, including current and aspiring practitioners, instructors, students, influencers, and anyone who uses computing technology in an impactful way. Additionally, the Code serves as a basis for remediation when violations occur. The Code includes principles formulated as statements of responsibility, based on the understanding that the public good is always the primary consideration. Each principle is supplemented by guidelines, which provide explanations to assist computing professionals in understanding and applying the principle.</p>
    <p>Section 1 outlines fundamental ethical principles that form the basis for the remainder of the Code. Section 2 addresses additional, more specific considerations of professional responsibility. Section 3 guides individuals who have a leadership role, whether in the workplace or in a volunteer professional capacity. Commitment to ethical conduct is required of every ACM member, ACM SIG member, ACM award recipient, and ACM SIG award recipient. Principles involving compliance with the Code are given in Section 4.</p>
    <p>The Code as a whole is concerned with how fundamental ethical principles apply to a computing professional's conduct. The Code is not an algorithm for solving ethical problems; rather it serves as a basis for ethical decision-making. When thinking through a particular issue, a computing professional may find that multiple principles should be taken into account, and that different principles will have different relevance to the issue. Questions related to these kinds of issues can best be answered by thoughtful consideration of the fundamental ethical principles, understanding that the public good is the paramount consideration. The entire computing profession benefits when the ethical decision-making process is accountable to and transparent to all stakeholders. Open discussions about ethical issues promote this accountability and transparency.</p>
    <h2>1. GENERAL ETHICAL PRINCIPLES.</h2>
    <p><em>A computing professional should...</em></p>
    <h3>1.1 Contribute to society and to human well-being, acknowledging that all people are stakeholders in computing.</h3>
    <p>This principle, which concerns the quality of life of all people, affirms an obligation of computing professionals, both individually and collectively, to use their skills for the benefit of society, its members, and the environment surrounding them. This obligation includes promoting fundamental human rights and protecting each individual's right to autonomy. An essential aim of computing professionals is to minimize negative consequences of computing, including threats to health, safety, personal security, and privacy. When the interests of multiple groups conflict, the needs of those less advantaged should be given increased attention and priority.</p>
    <p>Computing professionals should consider whether the results of their efforts will respect diversity, will be used in socially responsible ways, will meet social needs, and will be broadly accessible. They are encouraged to actively contribute to society by engaging in pro bono or volunteer work that benefits the public good.</p>
    <p>In addition to a safe social environment, human well-being requires a safe natural environment. Therefore, computing professionals should promote environmental sustainability both locally and globally.</p>
    <h3>1.2 Avoid harm.</h3>
    <p>In this document, "harm" means negative consequences, especially when those consequences are significant and unjust. Examples of harm include unjustified physical or mental injury, unjustified destruction or disclosure of information, and unjustified damage to property, reputation, and the environment. This list is not exhaustive.</p>
    <p>Well-intended actions, including those that accomplish assigned duties, may lead to harm. When that harm is unintended, those responsible are obliged to undo or mitigate the harm as much as possible. Avoiding harm begins with careful consideration of potential impacts on all those affected by decisions. When harm is an intentional part of the system, those responsible are obligated to ensure that the harm is ethically justified. In either case, ensure that all harm is minimized.</p>
    <p>To minimize the possibility of indirectly or unintentionally harming others, computing professionals should follow generally accepted best practices unless there is a compelling ethical reason to do otherwise. Additionally, the consequences of data aggregation and emergent properties of systems should be carefully analyzed. Those involved with pervasive or infrastructure systems should also consider Principle 3.7.</p>
    <p>A computing professional has an additional obligation to report any signs of system risks that might result in harm. If leaders do not act to curtail or mitigate such risks, it may be necessary to "blow the whistle" to reduce potential harm. However, capricious or misguided reporting of risks can itself be harmful. Before reporting risks, a computing professional should carefully assess relevant aspects of the situation.</p>
    <h3>1.3 Be honest and trustworthy.</h3>
    <p>Honesty is an essential component of trustworthiness. A computing professional should be transparent and provide full disclosure of all pertinent system capabilities, limitations, and potential problems to the appropriate parties. Making deliberately false or misleading claims, fabricating or falsifying data, offering or accepting bribes, and other dishonest conduct are violations of the Code.</p>
    <p>Computing professionals should be honest about their qualifications, and about any limitations in their competence to complete a task. Computing professionals should be forthright about any circumstances that might lead to either real or perceived conflicts of interest or otherwise tend to undermine the independence of their judgment. Furthermore, commitments should be honored.</p>
    <p>Computing professionals should not misrepresent an organization's policies or procedures, and should not speak on behalf of an organization unless authorized to do so.</p>
    <h3>1.4 Be fair and take action not to discriminate.</h3>
    <p>The values of equality, tolerance, respect for others, and justice govern this principle. Fairness requires that even careful decision processes provide some avenue for redress of grievances.</p>
    <p>Computing professionals should foster fair participation of all people, including those of underrepresented groups. Prejudicial discrimination on the basis of age, color, disability, ethnicity, family status, gender identity, labor union membership, military status, nationality, race, religion or belief, sex, sexual orientation, or any other inappropriate factor is an explicit violation of the Code. Harassment, including sexual harassment, bullying, and other abuses of power and authority, is a form of discrimination that, amongst other harms, limits fair access to the virtual and physical spaces where such harassment takes place.</p>
    <p>The use of information and technology may cause new, or enhance existing, inequities. Technologies and practices should be as inclusive and accessible as possible and computing professionals should take action to avoid creating systems or technologies that disenfranchise or oppress people. Failure to design for inclusiveness and accessibility may constitute unfair discrimination.</p>
    <h3>1.5 Respect the work required to produce new ideas, inventions, creative works, and computing artifacts.</h3>
    <p>Developing new ideas, inventions, creative works, and computing artifacts creates value for society, and those who expend this effort should expect to gain value from their work. Computing professionals should therefore credit the creators of ideas, inventions, work, and artifacts, and respect copyrights, patents, trade secrets, license agreements, and other methods of protecting authors' works.</p>
    <p>Both custom and the law recognize that some exceptions to a creator's control of a work are necessary for the public good. Computing professionals should not unduly oppose reasonable uses of their intellectual works. Efforts to help others by contributing time and energy to projects that help society illustrate a positive aspect of this principle. Such efforts include free and open source software and work put into the public domain. Computing professionals should not claim private ownership of work that they or others have shared as public resources.</p>
    <h3>1.6 Respect privacy.</h3>
    <p>The responsibility of respecting privacy applies to computing professionals in a particularly profound way. Technology enables the collection, monitoring, and exchange of personal information quickly, inexpensively, and often without the knowledge of the people affected. Therefore, a computing professional should become conversant in the various definitions and forms of privacy and should understand the rights and responsibilities associated with the collection and use of personal information.</p>
    <p>Computing professionals should only use personal information for legitimate ends and without violating the rights of individuals and groups. This requires taking precautions to prevent re-identification of anonymized data or unauthorized data collection, ensuring the accuracy of data, understanding the provenance of the data, and protecting it from unauthorized access and accidental disclosure. Computing professionals should establish transparent policies and procedures that allow individuals to understand what data is being collected and how it is being used, to give informed consent for automatic data collection, and to review, obtain, correct inaccuracies in, and delete their personal data.</p>
    <p>Only the minimum amount of personal information necessary should be collected in a system. The retention and disposal periods for that information should be clearly defined, enforced, and communicated to data subjects. Personal information gathered for a specific purpose should not be used for other purposes without the person's consent. Merged data collections can compromise privacy features present in the original collections. Therefore, computing professionals should take special care for privacy when merging data collections.</p>
    <h3>1.7 Honor confidentiality.</h3>
    <p>Computing professionals are often entrusted with confidential information such as trade secrets, client data, nonpublic business strategies, financial information, research data, pre-publication scholarly articles, and patent applications. Computing professionals should protect confidentiality except in cases where it is evidence of the violation of law, of organizational regulations, or of the Code. In these cases, the nature or contents of that information should not be disclosed except to appropriate authorities. A computing professional should consider thoughtfully whether such disclosures are consistent with the Code.</p>
    <h2>2. PROFESSIONAL RESPONSIBILITIES.</h2>
    <p><em>A computing professional should...</em></p>
    <h3>2.1 Strive to achieve high quality in both the processes and products of professional work.</h3>
    <p>Computing professionals should insist on and support high quality work from themselves and from colleagues. The dignity of employers, employees, colleagues, clients, users, and anyone else affected either directly or indirectly by the work should be respected throughout the process. Computing professionals should respect the right of those involved to transparent communication about the project. Professionals should be cognizant of any serious negative consequences affecting any stakeholder that may result from poor quality work and should resist inducements to neglect this responsibility.</p>
    <h3>2.2 Maintain high standards of professional competence, conduct, and ethical practice.</h3>
    <p>High quality computing depends on individuals and teams who take personal and group responsibility for acquiring and maintaining professional competence. Professional competence starts with technical knowledge and with awareness of the social context in which their work may be deployed. Professional competence also requires skill in communication, in reflective analysis, and in recognizing and navigating ethical challenges. Upgrading skills should be an ongoing process and might include independent study, attending conferences or seminars, and other informal or formal education. Professional organizations and employers should encourage and facilitate these activities.</p>
    <h3>2.3 Know and respect existing rules pertaining to professional work.</h3>
    <p>"Rules" here include local, regional, national, and international laws and regulations, as well as any policies and procedures of the organizations to which the professional belongs. Computing professionals must abide by these rules unless there is a compelling ethical justification to do otherwise. Rules that are judged unethical should be challenged. A rule may be unethical when it has an inadequate moral basis or causes recognizable harm. A computing professional should consider challenging the rule through existing channels before violating the rule. A computing professional who decides to violate a rule because it is unethical, or for any other reason, must consider potential consequences and accept responsibility for that action.</p>
    <h3>2.4 Accept and provide appropriate professional review.</h3>
    <p>High quality professional work in computing depends on professional review at all stages. Whenever appropriate, computing professionals should seek and utilize peer and stakeholder review. Computing professionals should also provide constructive, critical reviews of others' work.</p>
    <h3>2.5 Give comprehensive and thorough evaluations of computer systems and their impacts, including analysis of possible risks.</h3>
    <p>Computing professionals are in a position of trust, and therefore have a special responsibility to provide objective, credible evaluations and testimony to employers, employees, clients, users, and the public. Computing professionals should strive to be perceptive, thorough, and objective when evaluating, recommending, and presenting system descriptions and alternatives. Extraordinary care should be taken to identify and mitigate potential risks in machine learning systems. A system for which future risks cannot be reliably predicted requires frequent reassessment of risk as the system evolves in use, or it should not be deployed. Any issues that might result in major risk must be reported to appropriate parties.</p>
    <h3>2.6 Perform work only in areas of competence.</h3>
    <p>A computing professional is responsible for evaluating potential work assignments. This includes evaluating the work's feasibility and advisability, and making a judgment about whether the work assignment is within the professional's areas of competence. If at any time before or during the work assignment the professional identifies a lack of a necessary expertise, they must disclose this to the employer or client. The client or employer may decide to pursue the assignment with the professional after additional time to acquire the necessary competencies, to pursue the assignment with someone else who has the required expertise, or to forgo the assignment. A computing professional's ethical judgment should be the final guide in deciding whether to work on the assignment.</p>
    <h3>2.7 Foster public awareness and understanding of computing, related technologies, and their consequences.</h3>
    <p>As appropriate to the context and one's abilities, computing professionals should share technical knowledge with the public, foster awareness of computing, and encourage understanding of computing. These communications with the public should be clear, respectful, and welcoming. Important issues include the impacts of computer systems, their limitations, their vulnerabilities, and the opportunities that they present. Additionally, a computing professional should respectfully address inaccurate or misleading information related to computing.</p>
    <h3>2.8 Access computing and communication resources only when authorized or when compelled by the public good.</h3>
    <p>Individuals and organizations have the right to restrict access to their systems and data so long as the restrictions are consistent with other principles in the Code. Consequently, computing professionals should not access another's computer system, software, or data without a reasonable belief that such an action would be authorized or a compelling belief that it is consistent with the public good. A system being publicly accessible is not sufficient grounds on its own to imply authorization. Under exceptional circumstances a computing professional may use unauthorized access to disrupt or inhibit the functioning of malicious systems; extraordinary precautions must be taken in these instances to avoid harm to others.</p>
    <h3>2.9 Design and implement systems that are robustly and usably secure.</h3>
    <p>Breaches of computer security cause harm. Robust security should be a primary consideration when designing and implementing systems. Computing professionals should perform due diligence to ensure the system functions as intended, and take appropriate action to secure resources against accidental and intentional misuse, modification, and denial of service. As threats can arise and change after a system is deployed, computing professionals should integrate mitigation techniques and policies, such as monitoring, patching, and vulnerability reporting. Computing professionals should also take steps to ensure parties affected by data breaches are notified in a timely and clear manner, providing appropriate guidance and remediation.</p>
    <p>To ensure the system achieves its intended purpose, security features should be designed to be as intuitive and easy to use as possible. Computing professionals should discourage security precautions that are too confusing, are situationally inappropriate, or otherwise inhibit legitimate use.</p>
    <p>In cases where misuse or harm are predictable or unavoidable, the best option may be to not implement the system.</p>
    <h2>3. PROFESSIONAL LEADERSHIP PRINCIPLES.</h2>
    <p>Leadership may either be a formal designation or arise informally from influence over others. In this section, "leader" means any member of an organization or group who has influence, educational responsibilities, or managerial responsibilities. While these principles apply to all computing professionals, leaders bear a heightened responsibility to uphold and promote them, both within and through their organizations.</p>
    <p><em>A computing professional, especially one acting as a leader, should...</em></p>
    <h3>3.1 Ensure that the public good is the central concern during all professional computing work.</h3>
    <p>People—including users, customers, colleagues, and others affected directly or indirectly—should always be the central concern in computing. The public good should always be an explicit consideration when evaluating tasks associated with research, requirements analysis, design, implementation, testing, validation, deployment, maintenance, retirement, and disposal. Computing professionals should keep this focus no matter which methodologies or techniques they use in their practice.</p>
    <h3>3.2 Articulate, encourage acceptance of, and evaluate fulfillment of social responsibilities by members of the organization or group.</h3>
    <p>Technical organizations and groups affect broader society, and their leaders should accept the associated responsibilities. Organizations—through procedures and attitudes oriented toward quality, transparency, and the welfare of society—reduce harm to the public and raise awareness of the influence of technology in our lives. Therefore, leaders should encourage full participation of computing professionals in meeting relevant social responsibilities and discourage tendencies to do otherwise.</p>
    <h3>3.3 Manage personnel and resources to enhance the quality of working life.</h3>
    <p>Leaders should ensure that they enhance, not degrade, the quality of working life. Leaders should consider the personal and professional development, accessibility requirements, physical safety, psychological well-being, and human dignity of all workers. Appropriate human-computer ergonomic standards should be used in the workplace.</p>
    <h3>3.4 Articulate, apply, and support policies and processes that reflect the principles of the Code.</h3>
    <p>Leaders should pursue clearly defined organizational policies that are consistent with the Code and effectively communicate them to relevant stakeholders. In addition, leaders should encourage and reward compliance with those policies, and take appropriate action when policies are violated. Designing or implementing processes that deliberately or negligently violate, or tend to enable the violation of, the Code's principles is ethically unacceptable.</p>
    <h3>3.5 Create opportunities for members of the organization or group to grow as professionals.</h3>
    <p>Educational opportunities are essential for all organization and group members. Leaders should ensure that opportunities are available to computing professionals to help them improve their knowledge and skills in professionalism, in the practice of ethics, and in their technical specialties. These opportunities should include experiences that familiarize computing professionals with the consequences and limitations of particular types of systems. Computing professionals should be fully aware of the dangers of oversimplified approaches, the improbability of anticipating every possible operating condition, the inevitability of software errors, the interactions of systems and their contexts, and other issues related to the complexity of their profession—and thus be confident in taking on responsibilities for the work that they do.</p>
    <h3>3.6 Use care when modifying or retiring systems.</h3>
    <p>Interface changes, the removal of features, and even software updates have an impact on the productivity of users and the quality of their work. Leaders should take care when changing or discontinuing support for system features on which people still depend. Leaders should thoroughly investigate viable alternatives to removing support for a legacy system. If these alternatives are unacceptably risky or impractical, the developer should assist stakeholders' graceful migration from the system to an alternative. Users should be notified of the risks of continued use of the unsupported system long before support ends. Computing professionals should assist system users in monitoring the operational viability of their computing systems, and help them understand that timely replacement of inappropriate or outdated features or entire systems may be needed.</p>
    <h3>3.7 Recognize and take special care of systems that become integrated into the infrastructure of society.</h3>
    <p>Even the simplest computer systems have the potential to impact all aspects of society when integrated with everyday activities such as commerce, travel, government, healthcare, and education. When organizations and groups develop systems that become an important part of the infrastructure of society, their leaders have an added responsibility to be good stewards of these systems. Part of that stewardship requires establishing policies for fair system access, including for those who may have been excluded. That stewardship also requires that computing professionals monitor the level of integration of their systems into the infrastructure of society. As the level of adoption changes, the ethical responsibilities of the organization or group are likely to change as well. Continual monitoring of how society is using a system will allow the organization or group to remain consistent with their ethical obligations outlined in the Code. When appropriate standards of care do not exist, computing professionals have a duty to ensure they are developed.</p>
    <h2>4. COMPLIANCE WITH THE CODE.</h2>
    <p><em>A computing professional should...</em></p>
    <h3>4.1 Uphold, promote, and respect the principles of the Code.</h3>
    <p>The future of computing depends on both technical and ethical excellence. Computing professionals should adhere to the principles of the Code and contribute to improving them. Computing professionals who recognize breaches of the Code should take actions to resolve the ethical issues they recognize, including, when reasonable, expressing their concern to the person or persons thought to be violating the Code.</p>
    <h3>4.2 Treat violations of the Code as inconsistent with membership in the ACM.</h3>
    <p>Each ACM member should encourage and support adherence by all computing professionals regardless of ACM membership. ACM members who recognize a breach of the Code should consider reporting the violation to the ACM, which may result in remedial action as specified in the ACM's Code of Ethics and Professional Conduct Enforcement Policy.</p>
    <hr>
    <p><em>The Code and guidelines were developed by the ACM Code 2018 Task Force: Executive Committee Don Gotterbarn (Chair), Bo Brinkman, Catherine Flick, Michael S Kirkpatrick, Keith Miller, Kate Varansky, and Marty J Wolf. Members: Eve Anderson, Ron Anderson, Amy Bruckman, Karla Carter, Michael Davis, Penny Duquenoy, Jeremy Epstein, Kai Kimppa, Lorraine Kisselburgh, Shrawan Kumar, Andrew McGettrick, Natasa Milic-Frayling, Denise Oram, Simon Rogerson, David Shamma, Janice Sipior, Eugene Spafford, and Les Waguespack. The Task Force was organized by the ACM Committee on Professional Ethics. Significant contributions to the Code were also made by the broader international ACM membership. This Code and its guidelines were adopted by the ACM Council on June 22nd, 2018.</em></p>
</div><br>
<p></p>